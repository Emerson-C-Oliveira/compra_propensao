{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0. - STEP ONE - UNDERSTANDING THE BUSINESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The company is a health guarantee, looking to expand into Auto Insurance. Its business model is based on health policies, with an emphasis on quality of care. It faces fierce competition and sees expansion as a strategic opportunity. The project aims to predict customers' propensity to purchase Car Insurance, increasing revenue.\n",
    "\n",
    "For further studies, consult the project PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0. - STEP TWO - BUSINESS PROBLEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contracting insurer offers Health Insurance and now seek our help in creating a prediction model. The objective is to find out whether the beneficiaries of the previous year's health plan will also express interest in the new product that the company is launching, Auto Insurance.\n",
    "\n",
    "Just like health insurance, car plan customers will need to pay an annual amount, known as a premium, and in the event of a vehicle accident, the guarantee will provide compensation to the consumer.\n",
    "\n",
    "It depends on a survey of approximately 380,000 customers to assess their interest in adopting the new product. These responses were recorded in a database, along with other customer attributes.\n",
    "\n",
    "The product team selected 127,000 new customers who did not respond to the survey to participate in a campaign. During the campaign, these customers confirmed offers for the new product. However, the offer will be made by the sales team through telephone calls. The sales team has limited capacity and can only make 20,000 calls during the campaign period.\n",
    "\n",
    "In this context, the company is hiring a Data Science consultant to build a model that predicts whether or not the customer is interested in automobile insurance.\n",
    "\n",
    "Therefore, our goal is to create a model that is delivered in a list and descending order format, whose customers are most likely to purchase the product.\n",
    "\n",
    "This will allow the sales team to prioritize the people most interested in the product, optimizing the campaign and maximizing the company's revenue.\n",
    "\n",
    "Key insights into the most relevant attributes of specific customers when purchasing car insurance.\n",
    "• What percentage of specific customers purchasing auto insurance for the sales team will be able to reach 20,000 calls?\n",
    "• If the sales team's capacity increases to 40,000 calls, what percentage of customers interested in purchasing auto insurance will the sales team be able to contact?\n",
    "• How many calls does the sales team need to make to contact 80% of customers interested in purchasing auto insurance?\n",
    "\n",
    "The data available on the Kaggle website, each line represents a customer and each column contains some attributes that describe this customer, in addition to their response to the survey, in which they are interested or not in the new insurance product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0. - STEP THREE - DATA COLLECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. - Imports:\n",
    "In this section, all the necessary libraries and modules are imported. These libraries provide the required functionality for various data manipulation, visualization, and modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas            as pd\n",
    "import numpy             as np\n",
    "import scikitplot        as skplt\n",
    "import inflection\n",
    "import boruta            as bt\n",
    "import seaborn           as sns\n",
    "import pickle \n",
    "\n",
    "from tabulate             import tabulate\n",
    "from matplotlib           import pyplot          as plt\n",
    "from sklearn              import model_selection as ms \n",
    "from sklearn              import metrics         as m\n",
    "from scikitplot           import metrics         as mt\n",
    "from sklearn              import preprocessing   as pp\n",
    "from scipy                import stats           as ss\n",
    "from sklearn              import ensemble        as en\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, cohen_kappa_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "from IPython.core.display import Image\n",
    "from sklearn.ensemble     import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. - Helper Functions:\n",
    "This section contains several helper functions that will be used throughout the project to perform various tasks such as data pre-processing, error calculation and cross-validation. These functions help improve code organization and reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramer_v( x, y ):\n",
    "    # Calculate Cramer's V coefficient for two categorical variables.\n",
    "    cm = pd.crosstab( x, y ).values\n",
    "    n = cm.sum()\n",
    "    r, k = cm.shape\n",
    "    \n",
    "    chi2 = ss.chi2_contingency( cm )[0]\n",
    "    chi2corr = max( 0, chi2 - (k-1)*(r-1)/(n-1) )\n",
    "    \n",
    "    kcorr = k - (k-1)**2/(n-1)\n",
    "    rcorr = r - (r-1)**2/(n-1)\n",
    "    \n",
    "    return np.sqrt( (chi2corr/n) / ( min( kcorr-1, rcorr-1 ) ) )\n",
    "\n",
    "def corr_cramer_v(cat_attributes):\n",
    "    # Converte as colunas do DataFrame de atributos categóricos em uma lista\n",
    "    cat_attributes_list = cat_attributes.columns.tolist()\n",
    "\n",
    "    # Cria um dicionário vazio para armazenar a matriz de correlação\n",
    "    corr_dict = {}\n",
    "\n",
    "    # Loop para calcular a correlação entre todas as combinações de variáveis categóricas\n",
    "    for i in range(len(cat_attributes_list)):\n",
    "        # Lista vazia para armazenar as correlações da variável de referência\n",
    "        corr_list = []\n",
    "        for j in range(len(cat_attributes_list)):\n",
    "            # Seleciona a variável de referência e a variável alvo\n",
    "            ref = cat_attributes_list[i]\n",
    "            feat = cat_attributes_list[j]\n",
    "            # Calcula a correlação de Cramér V entre as duas variáveis\n",
    "            corr = cramer_v(cat_attributes[ref], cat_attributes[feat])\n",
    "            # Adiciona o valor de correlação à lista\n",
    "            corr_list.append(corr)\n",
    "        # Adiciona a lista de correlações ao dicionário, usando a variável de referência como chave\n",
    "        corr_dict[ref] = corr_list\n",
    "# GRAPHICS\n",
    "    # Retorna o dicionário contendo a matriz de correlação de Cramér V\n",
    "    return corr_dict\n",
    "\n",
    "def jupyter_settings():\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    # Settings to improve display in Jupyter Notebook.\n",
    "    plt.style.use('bmh') # Configures the plot style to 'bmh' (Bayesian Methods for Hackers).\n",
    "    plt.rcParams['figure.figsize'] = [25, 12] # Sets the default figure size to [25, 12].\n",
    "    plt.rcParams['font.size'] = 24 # Sets the default font size to 24.\n",
    "    display(HTML('<style>.container { width:100% !important; }</style>')) # Increases the width of the notebook to 100% of the screen width.\n",
    "    pd.options.display.max_columns = None # Sets the display options to show all columns of pandas dataframes.\n",
    "    pd.options.display.max_rows = None # Sets the display options to show all rows of pandas dataframes.\n",
    "    pd.set_option('display.expand_frame_repr', False) # Sets the option to not expand the frame when displaying large dataframes.\n",
    "    sns.set() # Sets the seaborn style for data visualization.\n",
    "\n",
    "    jupyter_settings()\n",
    "\n",
    "def graphic_percentage(ax,total):\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height,\n",
    "                '{:1.2f}'.format(height/total*100),\n",
    "                ha=\"center\") \n",
    "    plt.show()\n",
    "\n",
    "# ENCODING\n",
    "def frequency_enconding(df,column):\n",
    "    enconding=(df.groupby(column).size())/len(df)\n",
    "    return enconding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformations(x_validation):\n",
    "    \n",
    "    # Renomeie as colunas para snake_case\n",
    "    x_validation.columns = [inflection.underscore(col) for col in x_validation.columns]\n",
    "    \n",
    "    # Carregue os LabelEncoders salvos\n",
    "    gender_label_encoder = pickle.load(open('../propensity_score/parameter/gender_label_encoder.pkl', 'rb'))\n",
    "    vehicle_damage_label_encoder = pickle.load(open('../propensity_score/parameter/vehicle_damage_label_encoder.pkl', 'rb'))\n",
    "    \n",
    "    # Aplique o Label Encoding às colunas apropriadas\n",
    "    x_validation['gender'] = gender_label_encoder.transform(x_validation['gender'])\n",
    "    x_validation['vehicle_damage'] = vehicle_damage_label_encoder.transform(x_validation['vehicle_damage'])\n",
    "\n",
    "    # Carregue os MinMaxScalers salvos\n",
    "    mms_age = pickle.load(open('../propensity_score/parameter/mms_age_scaler.pkl', 'rb'))\n",
    "    mms_annual_premium = pickle.load(open('../propensity_score/parameter/annual_premium_scaler.pkl', 'rb'))\n",
    "    mms_vintage = pickle.load(open('../propensity_score/parameter/vintage_scaler.pkl', 'rb'))\n",
    "    \n",
    "    # Aplique o MinMaxScaler às colunas apropriadas\n",
    "    x_validation['age'] = mms_age.transform(x_validation[['age']])\n",
    "    x_validation['annual_premium'] = mms_annual_premium.transform(x_validation[['annual_premium']])\n",
    "    x_validation['vintage'] = mms_vintage.transform(x_validation[['vintage']])\n",
    "\n",
    "    # Carregue o dicionário de target encoding para 'region_code'\n",
    "    target_encode_region_code = pickle.load(open('../propensity_score/parameter/target_encode_region_code.pkl', 'rb'))\n",
    "    \n",
    "    # Mapeie os valores em 'region_code' com base nas taxas de resposta média\n",
    "    x_validation['region_code'] = x_validation['region_code'].map(target_encode_region_code)\n",
    "\n",
    "    # Crie as colunas one-hot para 'vehicle_age'\n",
    "    x_validation['below_1_year'] = x_validation['vehicle_age'].apply(lambda x: 1 if x == '< 1 Year' else 0)\n",
    "    x_validation['between_1_2_year'] = x_validation['vehicle_age'].apply(lambda x: 1 if x == '1-2 Year' else 0)\n",
    "    x_validation['over_2_years'] = x_validation['vehicle_age'].apply(lambda x: 1 if x == '> 2 Years' else 0)\n",
    "    x_validation.drop('vehicle_age', axis=1, inplace=True)\n",
    "\n",
    "    # Carregue o dicionário de frequency encoding para 'policy_sales_channel'\n",
    "    fe_policy_sales_channel = pickle.load(open('../propensity_score/parameter/fe_policy_sales_channel_scaler.pkl', 'rb'))\n",
    "    \n",
    "    # Mapeie os valores em 'policy_sales_channel' com base nas frequências relativas\n",
    "    x_validation['policy_sales_channel'] = x_validation['policy_sales_channel'].map(fe_policy_sales_channel)\n",
    "\n",
    "    x_validation = x_validation.fillna(0)\n",
    "\n",
    "    return x_validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(df,yhat_proba,target,perc=0.25):\n",
    "    k=int(np.floor(len(df)*perc))\n",
    "    df['score']=yhat_proba[:,1].tolist()\n",
    "    df=df.sort_values('score',ascending=False)\n",
    "    df=df.reset_index(drop=True)\n",
    "    df['ranking']=df.index+1\n",
    "    df['precision_at_k']=df[target].cumsum()/df['ranking']\n",
    "    return df.loc[k,'precision_at_k']\n",
    "\n",
    "def recall_at_k(df,yhat_proba,target,perc=0.25):\n",
    "    k=int(np.floor(len(df)*perc))\n",
    "    df['score']=yhat_proba[:,1].tolist()\n",
    "    df=df.sort_values('score',ascending=False)\n",
    "    df=df.reset_index(drop=True)\n",
    "    df['recall_at_k']=df[target].cumsum()/df[target].sum()\n",
    "    return df.loc[k,'recall_at_k']\n",
    "\n",
    "def topK_performance(df, probas, response, perc_list):\n",
    "    \"\"\"\n",
    "    Calculate conversion rate, target counts, cumulative gains and revenues.\n",
    "    Return a dataframe with the above metrics.\n",
    "    \"\"\"\n",
    "    results = pd.DataFrame(columns=['perc', 'conversion', 'target_total', 'target_at_k', 'cumulative_gains', 'revenue'])\n",
    "\n",
    "    for i, perc in enumerate(perc_list):\n",
    "        k = int(len(df) * perc)\n",
    "        df_tg = df.copy()\n",
    "        df_tg[response] = probas[0]\n",
    "        df_tg = df_tg.sort_values(by=response, ascending=False)\n",
    "        target_at_k = df_tg[response][:k].sum()\n",
    "        cumulative_gains = (df_tg[response][:k].sum() / df_tg[response].sum()) if df_tg[response].sum() > 0 else 0\n",
    "        revenue = target_at_k * 2000  # Assuming each conversion results in a revenue of 2000\n",
    "        results.loc[i] = [perc, df_tg[response][:k].sum() / len(df), df_tg[response].sum(), target_at_k, cumulative_gains, revenue]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification_and_ranking_metrics(model, x_val, y_val, y_val_prob=None, verbose=True):\n",
    "    # Treinar o modelo nos dados de treinamento\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Fazer previsões de classe nos dados de validação\n",
    "    y_val_pred = model.predict(x_val)\n",
    "\n",
    "    # Calcular métricas de classificação\n",
    "    accuracy = m.accuracy_score(y_val, y_val_pred)\n",
    "    balanced_accuracy = m.balanced_accuracy_score(y_val, y_val_pred)\n",
    "    kappa_score = m.cohen_kappa_score(y_val, y_val_pred)\n",
    "    precision = m.precision_score(y_val, y_val_pred)\n",
    "    recall = m.recall_score(y_val, y_val_pred)\n",
    "    f1_score = m.f1_score(y_val, y_val_pred)\n",
    "    matthew_corr = m.matthews_corrcoef(y_val, y_val_pred)\n",
    "\n",
    "    roc_auc = None\n",
    "    top_k_score = None\n",
    "    if y_val_prob is not None:\n",
    "        if len(np.unique(y_val)) > 2:\n",
    "            roc_auc = m.roc_auc_score(y_val, y_val_prob[:, 1])\n",
    "        num_classes = len(np.unique(y_val))\n",
    "        if num_classes > 2:\n",
    "            top_k_score = m.top_k_accuracy_score(y_val, y_val_prob, k=num_classes)\n",
    "\n",
    "    confusion_matrix = mt.plot_confusion_matrix(y_val, y_val_pred, normalize=False, figsize=(6, 6))\n",
    "    \n",
    "    # Calcular métricas de ranking\n",
    "    roc = m.roc_auc_score(y_val, y_val_pred)\n",
    "    knum = y_val.value_counts().count() - 1\n",
    "    top_k = m.top_k_accuracy_score(y_val, y_val_pred, k=knum)\n",
    "\n",
    "    ranking_metrics = {\n",
    "        'ROC AUC': roc,\n",
    "        'Top K Score': top_k\n",
    "    }\n",
    "    \n",
    "    metrics = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Balanced Accuracy': balanced_accuracy,\n",
    "        'Kappa Score': kappa_score,\n",
    "        'Precision Score': precision,\n",
    "        'Recall Score': recall,\n",
    "        'F1 Score': f1_score,\n",
    "        'Matthew Correlation Score': matthew_corr,\n",
    "        'ROC AUC': roc_auc,\n",
    "        'Top K Score': top_k_score,\n",
    "        'Confusion Matrix': confusion_matrix\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print('Classification Metrics:')\n",
    "        print('Accuracy:', accuracy)\n",
    "        print('Balanced Accuracy:', balanced_accuracy)\n",
    "        print('Kappa Score:', kappa_score)\n",
    "        print('Precision Score:', precision)\n",
    "        print('Recall Score:', recall)\n",
    "        print('F1 Score:', f1_score)\n",
    "        print('Matthew Correlation Score:', matthew_corr)\n",
    "        if roc_auc is not None:\n",
    "            print('ROC AUC:', roc_auc)\n",
    "        if top_k_score is not None:\n",
    "            print('Top K Score:', top_k_score)\n",
    "        \n",
    "        print('\\nRanking Metrics:')\n",
    "        print('ROC AUC:', roc)\n",
    "        print('Top K Score:', top_k)\n",
    "    \n",
    "    return metrics, ranking_metrics\n",
    "\n",
    "def ranking_class_metrics(model,y_val,yhat,verbose =1):\n",
    "    \n",
    "    model = pd.DataFrame([model]).T\n",
    "\n",
    "    #AUC_ROC\n",
    "    roc=m.roc_auc_score(y_val,yhat)\n",
    "    rocdf=pd.DataFrame([roc])\n",
    "    \n",
    "    #topk Score\n",
    "    knum=y_val.value_counts().count()-1\n",
    "    topk=m.top_k_accuracy_score(y_val,yhat,k=knum)\n",
    "    topkdf=pd.DataFrame([topk])    \n",
    "    \n",
    "    metrics = pd.concat([model,rocdf,topkdf]).T.reset_index()\n",
    "    metrics.columns=['Index','Model','ROC AUC','Top K Score']\n",
    "    metrics.drop(['Index'],axis=1)\n",
    "    if verbose ==1:\n",
    "        print('ROC AUC: {}'.format(roc))\n",
    "        print('Top K Score: {}'.format(topk))\n",
    "        #Classification Report\n",
    "        print(m.classification_report(y_val,yhat))\n",
    "        # Confision Matrix\n",
    "        mt.plot_confusion_matrix(y_val,yhat,normalize=False,figsize=(12,12))\n",
    "        \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_metrics(numerical_attributes):\n",
    "    #ct = central tendency\n",
    "    ct_mean = pd.DataFrame(numerical_attributes.apply(np.mean)).T\n",
    "    ct_median = pd.DataFrame(numerical_attributes.apply(np.median)).T\n",
    "\n",
    "    #d = dispersion\n",
    "    d_std = pd.DataFrame(numerical_attributes.apply(np.std)).T\n",
    "    d_min = pd.DataFrame(numerical_attributes.apply(min)).T\n",
    "    d_max = pd.DataFrame(numerical_attributes.apply(max)).T\n",
    "    d_range = pd.DataFrame(numerical_attributes.apply(lambda x: x.max() - x.min())).T\n",
    "    d_skew = pd.DataFrame(numerical_attributes.apply(lambda x: x.skew())).T\n",
    "    d_kurtosis = pd.DataFrame(numerical_attributes.apply(lambda x: x.kurtosis())).T\n",
    "\n",
    "    na_resume = pd.concat([d_min,d_max,d_range,ct_mean,ct_median,d_std,d_skew,d_kurtosis]).T.reset_index()\n",
    "    na_resume.columns=['Attributes','Min','Max','Range','Mean','Median','STD','Skew','Kurtosis']\n",
    "\n",
    "    return na_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(model, x, y, num_folds=5):\n",
    "    # Inicialize as listas para armazenar as métricas para cada fold\n",
    "    accuracy_scores = []\n",
    "    balanced_accuracy_scores = []\n",
    "    kappa_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    matthews_corr_scores = []\n",
    "    roc_auc_scores = []\n",
    "\n",
    "    # Divida os dados usando validação cruzada estratificada\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_index, val_index in skf.split(x, y):\n",
    "        x_train_fold, x_val_fold = x.iloc[train_index], x.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        # Treine o modelo no fold de treinamento\n",
    "        model.fit(x_train_fold, y_train_fold)\n",
    "\n",
    "        # Faça previsões no fold de validação\n",
    "        y_val_pred = model.predict(x_val_fold)\n",
    "\n",
    "        # Calcule as métricas e as armazene nas listas\n",
    "        accuracy_scores.append(accuracy_score(y_val_fold, y_val_pred))\n",
    "        balanced_accuracy_scores.append(balanced_accuracy_score(y_val_fold, y_val_pred))\n",
    "        kappa_scores.append(cohen_kappa_score(y_val_fold, y_val_pred))\n",
    "        precision_scores.append(precision_score(y_val_fold, y_val_pred))\n",
    "        recall_scores.append(recall_score(y_val_fold, y_val_pred))\n",
    "        f1_scores.append(f1_score(y_val_fold, y_val_pred))\n",
    "        matthews_corr_scores.append(matthews_corrcoef(y_val_fold, y_val_pred))\n",
    "\n",
    "        # Calcule a probabilidade de classe 1 (se aplicável) e a ROC AUC\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_val_prob = model.predict_proba(x_val_fold)[:, 1]\n",
    "            roc_auc_scores.append(roc_auc_score(y_val_fold, y_val_prob))\n",
    "        else:\n",
    "            roc_auc_scores.append(None)\n",
    "\n",
    "    # Calcule as médias das métricas para todos os folds\n",
    "    avg_accuracy = np.mean(accuracy_scores)\n",
    "    avg_balanced_accuracy = np.mean(balanced_accuracy_scores)\n",
    "    avg_kappa = np.mean(kappa_scores)\n",
    "    avg_precision = np.mean(precision_scores)\n",
    "    avg_recall = np.mean(recall_scores)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    avg_matthews_corr = np.mean(matthews_corr_scores)\n",
    "    avg_roc_auc = np.mean([score for score in roc_auc_scores if score is not None])\n",
    "\n",
    "    # Armazene as médias das métricas em um dicionário\n",
    "    metrics = {\n",
    "        'Accuracy': avg_accuracy,\n",
    "        'Balanced Accuracy': avg_balanced_accuracy,\n",
    "        'Kappa Score': avg_kappa,\n",
    "        'Precision Score': avg_precision,\n",
    "        'Recall Score': avg_recall,\n",
    "        'F1 Score': avg_f1,\n",
    "        'Matthew Correlation Score': avg_matthews_corr,\n",
    "        'ROC AUC': avg_roc_auc\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Notation\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. - Loading Data:\n",
    "In this section, the code is focused on loading the data into pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv( '../propensity_score/data/raw/train.csv' )\n",
    "\n",
    "df_test = pd.read_csv( '../propensity_score/data/raw/test.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. - Data Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Id:Unique ID for the customer\n",
    "\n",
    "Gender: Gender of the customer\n",
    "\n",
    "Age: Age of the customer\n",
    "\n",
    "Driving License: 0 : Customer does not have DL, 1 : Customer already has DL\n",
    "\n",
    "Region Code: Unique code for the region of the customer\n",
    "\n",
    "Previously Insured: 1 : Customer already has Vehicle Insurance, 0 : Customer doesn't have Vehicle Insurance\n",
    "\n",
    "Vehicle Age: Age of the Vehicle\n",
    "\n",
    "Vehicle Damage: 1 : Customer got his/her vehicle damaged in the past. 0 : Customer didn't get his/her vehicle damaged in the past.\n",
    "\n",
    "Anual Premium: The amount customer needs to pay as premium in the year\n",
    "\n",
    "Policy sales channel: Anonymized Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.\n",
    "\n",
    "Vintage: Number of Days, Customer has been associated with the company\n",
    "\n",
    "Response: 1 : Customer is interested, 0 : Customer is not interested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0. - STEP FOUR - DATA DRESCRIPTION\n",
    "In this section, the first step of the analysis is carried out, which consists of the description of the data. Here, data characteristics will be explored, such as descriptive statistics, variable types, missing values, among other relevant information about the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOD PRATICE: CREATING A COPY\n",
    "# Making a copy of the DataFrame is a good practice as it helps avoid unintentional modifications to the original data during data exploration, preprocessing, and modeling.\n",
    "\n",
    "# Create a copy of the DataFrame df_raw and store it in df1.\n",
    "df1 = df_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. - Rename Columns:\n",
    "In this section, the columns of the dataset will be renamed according to the analysis requirements. The renaming technique used here is \"snake_case,\" which converts all column names to lowercase and separates them by underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original column names before renaming\n",
    "cols_old = ['id', 'Gender', 'Age', 'Driving_License', 'Region_Code',\n",
    "       'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage', 'Annual_Premium',\n",
    "       'Policy_Sales_Channel', 'Vintage', 'Response']\n",
    "\n",
    "# Function to convert names to snake_case\n",
    "snakecase = lambda x: inflection.underscore(x) \n",
    "\n",
    "# New column names after renaming\n",
    "cols_new = list(map(snakecase, cols_old))\n",
    "\n",
    "# Rename the DataFrame columns using the new names\n",
    "df1.columns = cols_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. - Data Dimensions:\n",
    "In this section, the dimensions of the dataset will be presented, that is, the number of rows (samples) and columns (attributes). This step is important for us to have an overview of the amount of information available and to understand the size of the data set we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of rows in the DataFrame df1\n",
    "print('Number of Rows: {}'.format(df1.shape[0]))\n",
    "\n",
    "# Print the number of columns in the DataFrame df1\n",
    "print('Number of Columns: {}'.format(df1.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. - Data Types:\n",
    "In this section, we'll explore the data types of each column in the DataFrame df1. Understanding data types is crucial for data manipulation, analysis, and pre-processing. Different data types represent different types of information, such as numeric values, strings, dates, or categorical variables.\n",
    "\n",
    "By examining data types, we can identify any inconsistencies or potential issues in the dataset, as well as determine which columns may require type conversion or additional data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. - Check  Missing Values (NAs):\n",
    "In this section, we check for missing values ​​in the DataFrame df1 and handle them accordingly. We will apply appropriate strategies to fill in the missing data, in line with Rossmann's business model, to ensure data consistency and integrity for further analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK NAs\n",
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Descriptive Statistical:\n",
    "In this section, we will perform descriptive statistical analysis on the dataset to gain insights into the data's distribution and central tendencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting numerical attributes (int64, float64, and int32) from the dataframe df1\n",
    "numerical_columns = df1.select_dtypes(include=['int64', 'float64', 'int32'])\n",
    "\n",
    "# Selecting categorical attributes (excluding int64, float64, datetime64[ns], and int32) from the dataframe df1\n",
    "categorical_columns = df1.select_dtypes(exclude=['int64', 'float64', 'datetime64[ns]', 'int32'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1. Numerical Attributes:\n",
    "In this section, we will perform descriptive statistical analysis on the numerical attributes of the dataset. The goal is to gain insights into the distribution, central tendency, and dispersion of these numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHTS:\n",
    "+ Cerca de 12,25% de todos os consumidores responderam ter interesse em adquirir o seguro de automóvel. \n",
    "+ Cerca de 54,17% de todos os consumidores não possuem seguro de automóvel, enquanto 45,82% já possuem. \n",
    "+ O valor médio do seguro saúde é de 31.000,00.\n",
    "+ Todos os consumidores possuem menos de 1 ano no seguro saúde. Sendo que 75% tinham 227 dias. Isso pode significar duas coisas, a primeira é que a base parece ser renovada e um \"novo seguro\" é emitido ano após ano, não existindo continuidade contratual maior do que 12 meses, mesmo que o cliente deseje continuar segurado, quando ele renova um novo relacionamento é emitido (na base de dados). E o segundo ponto, é que talvez poder-se-iamos  tratar 75% da base como clientes fies, ou seja, ao menos 75% de renovação teríamos todo o ano. A ser estudado em uma segundo Ciclo. \n",
    "+ A maioria dos consumidores têm entre 25 a 49 anos.\n",
    "+ A maioria absoluta dos clientes possuem carteira de motorista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_metrics(numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns.hist(bins=25, figsize=(15, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um grid de subplots com 1 linha e 3 colunas\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Gráfico de pizza para \"gender\"\n",
    "value_counts = df1['response'].value_counts()\n",
    "axs[0].pie(value_counts, labels=value_counts.index, autopct=lambda p: f'{p:.1f}% ({int(p * sum(value_counts) / 100)})', startangle=140)\n",
    "axs[0].set_title('Distribuição da variável \"response\"')\n",
    "\n",
    "# Gráfico de pizza para \"vehicle_age\"\n",
    "value_counts = df1['previously_insured'].value_counts()\n",
    "axs[1].pie(value_counts, labels=value_counts.index, autopct=lambda p: f'{p:.1f}% ({int(p * sum(value_counts) / 100)})', startangle=140)\n",
    "axs[1].set_title('Distribuição da variável \"previously_insured\"')\n",
    "\n",
    "# Gráfico de pizza para \"vehicle_damage\"\n",
    "value_counts = df1['driving_license'].value_counts()\n",
    "axs[2].pie(value_counts, labels=value_counts.index, autopct=lambda p: f'{p:.1f}% ({int(p * sum(value_counts) / 100)})', startangle=140)\n",
    "axs[2].set_title('Distribuição da variável \"driving_license\"')\n",
    "\n",
    "# Ajusta a posição dos subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Exibe os subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste o tamanho da figura\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.countplot(data=df1, x='region_code')\n",
    "plt.title('Contagem por region_code')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.countplot(data=df1, x='policy_sales_channel')\n",
    "plt.title('Contagem por policy_sales_channel')\n",
    "\n",
    "# Melhore a disposição dos subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2. Categorical Attributes:\n",
    "In this section, we will analyze the categorical attributes present in dataframe df1. These attributes are non-numeric variables that represent different categories or groups. We will do an exploratory analysis of these attributes to understand their distribution and frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHTS:\n",
    "+ A maioria dos clientes são homens.\n",
    "+ A maioria dos veículos possuem menos de 2 anos. \n",
    "+ 50,49% dos consumidores tiveram danos em seus veículos no passado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the lambda function to count the number of unique values in each categorical attribute\n",
    "categorical_columns.apply(lambda x: x.unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe values in each categorical attribute\n",
    "categorical_columns.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um grid de subplots com 1 linha e 3 colunas\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Gráfico de pizza para \"gender\"\n",
    "value_counts = df1['gender'].value_counts()\n",
    "axs[0].pie(value_counts, labels=value_counts.index, autopct=lambda p: f'{p:.1f}% ({int(p * sum(value_counts) / 100)})', startangle=140)\n",
    "axs[0].set_title('Distribuição da variável \"gender\"')\n",
    "\n",
    "# Gráfico de pizza para \"vehicle_age\"\n",
    "value_counts = df1['vehicle_age'].value_counts()\n",
    "axs[1].pie(value_counts, labels=value_counts.index, autopct=lambda p: f'{p:.1f}% ({int(p * sum(value_counts) / 100)})', startangle=140)\n",
    "axs[1].set_title('Distribuição da variável \"vehicle_age\"')\n",
    "\n",
    "# Gráfico de pizza para \"vehicle_damage\"\n",
    "value_counts = df1['vehicle_damage'].value_counts()\n",
    "axs[2].pie(value_counts, labels=value_counts.index, autopct=lambda p: f'{p:.1f}% ({int(p * sum(value_counts) / 100)})', startangle=140)\n",
    "axs[2].set_title('Distribuição da variável \"vehicle_damage\"')\n",
    "\n",
    "# Ajusta a posição dos subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Exibe os subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0. - STEP FIVE - DATA CLEANING AND PRE-PREPARATION\n",
    "In this step, we conducted data cleaning, preprocessing,  to enhance data quality and prepare it for the sales forecast model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOD PRATICE: \n",
    "# Create a copy of the DataFrame df2 it in df1.\n",
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. - Handle Missing Values (NAs): Não é preciso para este conjunto de dados\n",
    " We will apply appropriate strategies to fill in the missing data, in line with Rossmann's business model, to ensure data consistency and integrity for further analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. - Change Data Types: Não é preciso para este conjunto de dados\n",
    "In this section, we will convert the data types of certain columns to more appropriate formats, optimizing memory usage, and ensuring consistency in data representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. - Variable filtering: Não é preciso para este conjunto de dados\n",
    "In this section, we will filter the variables to select only the relevant features for our analysis and modeling. This process aims to reduce dimensionality and focus on the most important attributes that can significantly impact sales prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1. - Line filtering: Não é preciso para este conjunto de dados\n",
    "We will remove data from stores that are closed since they would not have any sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2. - Column selection: Não é preciso para este conjunto de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. - Mental Map of Hypotheses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('img/MindMapHyphoteses.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. - Formulation of Hypotheses:\n",
    "In this section, we will formulate hypotheses that will help guide our analyzes and decision-making throughout the project. Hypotheses are assumptions that we can test using available data. They are essential to guide our exploration of the data and help us find patterns and insights relevant to the problem at hand.\n",
    "\n",
    "### 5.5.1. - Hyphoteses related to the Customers:\n",
    "1. Women buy more vehicle insurance than men. - As mulheres compram mais seguros de veículos que os homens.\n",
    "2. Men tend to claim insurance more often. - Homens tendem a acionar mais vezes o seguro.\n",
    "3. Customers without a driver's license are not interested in car insurance. - Clientes sem carteira de motorista não possui interesse em seguro de automóvel.\n",
    "4. Customers over 30 are more interested in car insurance. - Clientes com mais de 30 anos possui mais interesse em seguross de automóvel.\n",
    "\n",
    "\n",
    "### 5.5.2. - Hypotheses related to the Vehicle:\n",
    "1. Customers who have a car less than 2 years old are more interested in car insurance. - Clientes que possuem carro com menos de 2 anos possui mais interesse no seguro de automóvel.\n",
    "\n",
    "### 5.5.3. - Hypotheses related to Insurance:\n",
    "1. Customers who pay a higher annual premium are more interested in vehicle insurance. - Clientes que pagam um maior prêmio anual possui mais interesse no seguro de veículos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0. - STEP SIX - EXPLORATORY DATA ANALYSIS (EDA)\n",
    "Exploratory Data Analysis (EDA) is a critical step in any data science and predictive modeling project. In this phase, we will explore and examine the dataset in more detail to gain insights, understand patterns, identify trends and anomalies, and prepare the data for predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame df2 it in df3.\n",
    "df3 = df2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. - Practical Feature Engineering:\n",
    "In this section, we will perform feature engineering on the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df3):\n",
    "    df3['risk_age']= df3['age'].apply(lambda x: 0 if x>25 else 1)    \n",
    "    \n",
    "    df3['age_insured'] = ((df3['age'] >= 32) & (df3['age'] <= 52) & (df3['previously_insured'] == 0)).astype(int)\n",
    "    \n",
    "    return  df3\n",
    "\n",
    "df3 = feature_engineering(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie uma tabela de contingência entre 'risk_age' e 'response'\n",
    "contingency_table = pd.crosstab(df3['risk_age'], df3['response'])\n",
    "\n",
    "# Plote um gráfico de pizza\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.pie(contingency_table.loc[1], labels=contingency_table.columns, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribuição de response para risk_age = 1')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.legend(title='Response', labels=['0', '1'])\n",
    "\n",
    "# Adicione um insight abaixo do gráfico\n",
    "plt.text(0.5, -0.2, \"Insight: A maioria dos clientes de risco alto (1) não tem interesse no seguro (0)\",\n",
    "         ha='center', transform=plt.gca().transAxes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a contagem dos valores\n",
    "value_counts = df3['age_insured'].value_counts()\n",
    "\n",
    "# Cria um gráfico de pizza\n",
    "plt.figure(figsize=(3, 3))  # Define o tamanho do gráfico\n",
    "plt.pie(value_counts, labels=value_counts.index, autopct=lambda p: f'{p:.1f}% ({int(p * sum(value_counts) / 100)})', startangle=140)\n",
    "\n",
    "# Define o título do gráfico\n",
    "plt.title('Distribuição da variável \"age_insured\"')\n",
    "\n",
    "# Adicione um insight abaixo do gráfico\n",
    "plt.text(0.5, -0.2, \"Insight: 25,5% da base possui entre 32 a 52 anos e não possuem seguro anterior\",\n",
    "         ha='center', transform=plt.gca().transAxes)\n",
    "\n",
    "# Exibe o gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. - Univariate Analysis:\n",
    "In this section, we will perform univariate analysis on the dataset. Univariate analysis involves examining each variable individually to understand its distribution, central tendency, dispersion, and any potential outliers. This analysis helps us gain insights into the characteristics of each variable and identify any data issues that may need to be addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1. - Reponse Variable\n",
    "In this subsection, we will focus on the response variable, which is the target variable we aim to predict or explain. In our case, the response variable is the 'response' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a contagem dos valores\n",
    "value_counts = df3  ['response'].value_counts()\n",
    "\n",
    "# Calcula a contagem dos valores normalizados\n",
    "value_percentages = df3 ['response'].value_counts(normalize=True)\n",
    "\n",
    "# Cria um gráfico de pizza\n",
    "plt.figure(figsize=(4, 4))  # Define o tamanho do gráfico\n",
    "plt.pie(value_counts, labels=value_counts.index, autopct=lambda p: f'{p:.1f}% ({int(p * sum(value_counts) / 100)})', startangle=140)\n",
    "plt.legend(title='Response', labels=['0', '1'])\n",
    "\n",
    "# Define o título do gráfico\n",
    "plt.title('Distribuição da variável \"response\"')\n",
    "\n",
    "# Exibe o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2. - Numerical Variable:\n",
    "In this section, we will perform a univariate analysis of numeric variables. Numerical variables are those that represent quantitative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incluindo a coluna risk_age nos atributos categóricos\n",
    "numerical_columns = df3.select_dtypes(include=['int64', 'int32','float64'])\n",
    "\n",
    "# Plot histograms for all numerical columns\n",
    "numerical_columns.hist(bins=25, figsize=(15, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3. - Categorical Variable:\n",
    "In this section, we will explore the categorical variables in our dataset. Categorical variables are non-numeric and represent different categories or groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o número de linhas e colunas para o grid de subplots\n",
    "num_rows = 1  # Uma linha\n",
    "num_cols = 3  # Três colunas (um para cada gráfico)\n",
    "\n",
    "# Cria um grid de subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5))\n",
    "\n",
    "# Plota o primeiro gráfico (Comparação de Gênero com Resposta)\n",
    "sns.countplot(data=df3, x='response', hue='gender', ax=axes[0])\n",
    "axes[0].set_title('Comparação de Gênero com Resposta')\n",
    "axes[0].legend(title='Gênero')\n",
    "\n",
    "# Plota o segundo gráfico (Comparação de Faixa de Idade do Veículo com Resposta)\n",
    "sns.countplot(data=df3, x='response', hue='vehicle_age', ax=axes[1])\n",
    "axes[1].set_title('Comparação de Faixa de Idade do Veículo com Resposta')\n",
    "axes[1].legend(title='Faixa de Idade do Veículo')\n",
    "\n",
    "# Plota o terceiro gráfico (Comparação de Danos no Veículo com Resposta)\n",
    "sns.countplot(data=df3, x='response', hue='vehicle_damage', ax=axes[2])\n",
    "axes[2].set_title('Comparação de Danos no Veículo com Resposta')\n",
    "axes[2].legend(title='Danos no Veículo')\n",
    "\n",
    "# Ajusta a posição dos subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Exibe os gráficos\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o número de linhas e colunas para o grid de subplots\n",
    "num_rows = 2  # Três linhas (uma para cada comparação)\n",
    "num_cols = 2  # Duas colunas (uma para a tabela e outra para o gráfico)\n",
    "\n",
    "# Cria um grid de subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 8))\n",
    "\n",
    "# Tabela de contingência para comparar gender e vehicle_age\n",
    "contingency_table = pd.crosstab(df3['gender'], df3['vehicle_age'])\n",
    "print(\"Tabela de Contingência (gender vs vehicle_age):\")\n",
    "print(contingency_table)\n",
    "\n",
    "# Gráfico de barras para gender vs vehicle_age\n",
    "sns.countplot(data=df3, x='gender', hue='vehicle_age', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Contagem de Gênero por Faixa de Idade do Veículo')\n",
    "\n",
    "# Tabela de contingência para comparar gender e vehicle_damage\n",
    "contingency_table = pd.crosstab(df3['gender'], df3['vehicle_damage'])\n",
    "print(\"\\nTabela de Contingência (gender vs vehicle_damage):\")\n",
    "print(contingency_table)\n",
    "\n",
    "# Gráfico de barras para gender vs vehicle_damage\n",
    "sns.countplot(data=df3, x='gender', hue='vehicle_damage', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Contagem de Gênero por Danos no Veículo')\n",
    "\n",
    "# Tabela de contingência para comparar vehicle_damage e vehicle_age\n",
    "contingency_table = pd.crosstab(df3['vehicle_damage'], df3['vehicle_age'])\n",
    "print(\"Tabela de Contingência (vehicle_damage vs vehicle_age):\")\n",
    "print(contingency_table)\n",
    "\n",
    "# Gráfico de barras para vehicle_damage vs vehicle_age\n",
    "sns.countplot(data=df3, x='vehicle_damage', hue='vehicle_age', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Contagem de Danos no Veículo por Faixa de Idade do Veículo')\n",
    "\n",
    "# Ajusta a posição dos subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Exibe os subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. - Bivariate Analysis:\n",
    "In this section, we will perform a bivariate analysis, exploring the relationship between two variables. Let's investigate how numerical and categorical variables behave together and how they can influence response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1. Age:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie uma grade de subplots com 1 linha e 3 colunas\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Primeiro gráfico: boxplot\n",
    "sns.boxplot(x='response', y='age', data=df3, ax=axes[0])\n",
    "axes[0].set_title('Boxplot')\n",
    "\n",
    "# Segundo gráfico: histograma para response==0\n",
    "aux00 = df3.loc[df3['response'] == 0, 'age']\n",
    "sns.histplot(aux00, ax=axes[1], kde=True, color='blue')\n",
    "axes[1].set_title('Histograma para response==0')\n",
    "\n",
    "# Terceiro gráfico: histograma para response==1\n",
    "aux01 = df3.loc[df3['response'] == 1, 'age']\n",
    "sns.histplot(aux01, ax=axes[2], kde=True, color='green')\n",
    "axes[2].set_title('Histograma para response==1')\n",
    "\n",
    "# Ajuste o layout dos gráficos\n",
    "plt.tight_layout()\n",
    "\n",
    "# Exiba os gráficos\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2. Annual Premium:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie uma grade de subplots com 1 linha e 3 colunas\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Primeiro gráfico: boxplot\n",
    "sns.boxplot( x='response', y='annual_premium', data=df3, ax=axes[0])\n",
    "axes[0].set_title('Boxplot')\n",
    "\n",
    "# Filtrando os dados da coluna Annual Premium para uma melhor visualização\n",
    "df3 = df3[(df3['annual_premium'] > 10000) & (df3['annual_premium'] < 100000)]\n",
    "\n",
    "# Segundo gráfico: histograma para response==0\n",
    "aux00 = df3.loc[df3['response'] == 0, 'annual_premium']\n",
    "sns.histplot(aux00, ax=axes[1], kde=True, color='blue')\n",
    "axes[1].set_title('Histograma para response==0')\n",
    "\n",
    "# Terceiro gráfico: histograma para response==1\n",
    "aux01 = df3.loc[df3['response'] == 1, 'annual_premium']\n",
    "sns.histplot(aux01, ax=axes[2], kde=True, color='green')\n",
    "axes[2].set_title('Histograma para response==1')\n",
    "\n",
    "# Ajuste o layout dos gráficos\n",
    "plt.tight_layout()\n",
    "\n",
    "# Exiba os gráficos\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.3. Driving License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a contagem dos valores\n",
    "value_counts = df3['driving_license'].value_counts()\n",
    "\n",
    "# Plote um gráfico de pizza\n",
    "plt.figure(figsize=(6, 6))  # Define o tamanho do gráfico\n",
    "plt.pie(value_counts, labels=value_counts.index, autopct=lambda p: f'{p:.1f}% ({int(p * sum(value_counts) / 100)})', startangle=140)\n",
    "\n",
    "# Adicione uma legenda à variável resposta\n",
    "plt.legend(title='Response', labels=['1: Sim', '0: Não'], loc='best')\n",
    "\n",
    "# Define o título do gráfico\n",
    "plt.title('Distribuição da Coluna \"driving_license\" pela Variável Resposta')\n",
    "\n",
    "# Exibe o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.4. Region Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux0 = df3[['id', 'region_code', 'response']].groupby( ['region_code', 'response'] ).count().reset_index()\n",
    "#aux0 = aux0[(aux0['id'] > 1000) & (aux0['id'] < 20000)]\n",
    "\n",
    "sns.scatterplot( x='region_code', y='id', hue='response', data=aux0 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.5. Previously Insurance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHT:\n",
    "+ Quem já tem seguro de veículo, não possui interesse no novo.\n",
    "+ Dos que não possuem seguro de veículo ainda, 22,5% diz ter interesse.\n",
    "\n",
    "- A diferença nas probabilidades entre esses dois grupos é significativa, indicando que a variável \"previously_insured\" pode ser um preditor importante para a variável \"response\". Clientes não segurados têm uma probabilidade muito maior de se interessar por um seguro do que aqueles que já foram segurados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux0 = df3[['id', 'previously_insured', 'response']].groupby( ['previously_insured', 'response'] ).count().reset_index()\n",
    "#aux0 = aux0[(aux0['id'] > 1000) & (aux0['id'] < 20000)]\n",
    "\n",
    "sns.barplot( x='previously_insured', y='id', hue='response', data=aux0 );\n",
    "\n",
    "pd.crosstab(df3['previously_insured'], df3['response'] ).apply( lambda x: x / x.sum(), axis=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.6. Vehicle Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[['id','vehicle_age', 'response']].groupby( ['vehicle_age', 'response'] ).count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.7. Vehicle Damage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie um DataFrame auxiliar para o primeiro gráfico de barras\n",
    "aux1 = df3[['vehicle_damage', 'response']].groupby(['vehicle_damage', 'response']).size().reset_index().rename(columns={0: 'qtd'})\n",
    "\n",
    "# Crie um DataFrame auxiliar para o segundo gráfico de barras\n",
    "aux2 = df3[df3['response'] == 1]\n",
    "aux2 = aux2[['vehicle_damage', 'response']].groupby(['vehicle_damage', 'response']).size().reset_index().rename(columns={0: 'qtd'})\n",
    "\n",
    "# Crie uma figura com dois subplots lado a lado\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Primeiro subplot (gráfico de barras)\n",
    "sns.barplot(data=aux1, x='vehicle_damage', y='qtd', hue='response', ax=axs[0])\n",
    "total1 = sum(aux1['qtd'])\n",
    "axs[0].set_title('Gráfico 1: Vehicle Damage por Response')\n",
    "axs[0].set_xlabel('Vehicle Damage')\n",
    "axs[0].set_ylabel('Contagem')\n",
    "axs[0].legend(title='Response')\n",
    "axs[0].set_ylim(0, max(aux1['qtd']) + 100)  # Defina limites y para melhor visualização\n",
    "axs[0].set_xticklabels(axs[0].get_xticklabels(), rotation=45)\n",
    "\n",
    "# Segundo subplot (gráfico de barras)\n",
    "sns.barplot(data=aux2, x='vehicle_damage', y='qtd', ax=axs[1])\n",
    "total2 = sum(aux2['qtd'])\n",
    "axs[1].set_title('Gráfico 2: Vehicle Damage para Response=1')\n",
    "axs[1].set_xlabel('Vehicle Damage')\n",
    "axs[1].set_ylabel('Contagem')\n",
    "axs[1].set_ylim(0, max(aux2['qtd']) + 100)  # Defina limites y para melhor visualização\n",
    "axs[1].set_xticklabels(axs[1].get_xticklabels(), rotation=45)\n",
    "\n",
    "# Exiba os gráficos\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.8. Policy Sales Chamnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = df3[['policy_sales_channel', 'response']].groupby( 'policy_sales_channel' ).sum().reset_index()\n",
    "sns.barplot( x='response', y='policy_sales_channel', data=aux );\n",
    "\n",
    "aux01 = df3[['policy_sales_channel', 'response']].groupby( 'policy_sales_channel' ).sum().reset_index()\n",
    "aux02 = df3[['id', 'policy_sales_channel']].groupby( 'policy_sales_channel' ).size().reset_index().rename( columns={0:'total_responses'})\n",
    "\n",
    "aux = pd.merge( aux01, aux02, how='inner', on='policy_sales_channel' )\n",
    "aux.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.9. Vintage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie uma grade de subplots com 1 linha e 3 colunas\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Primeiro gráfico: histograma para response==0\n",
    "aux0 = df3.loc[df3['response'] == 0, 'vintage']\n",
    "sns.histplot(aux0, ax=axes[0], kde=True, color='blue')\n",
    "axes[0].set_title('Histograma para response==0')\n",
    "\n",
    "# Segundo gráfico: histograma para response==1\n",
    "aux1 = df3.loc[df3['response'] == 1, 'vintage']\n",
    "sns.histplot(aux1, ax=axes[1], kde=True, color='green')\n",
    "axes[1].set_title('Histograma para response==1')\n",
    "\n",
    "# Terceiro gráfico: gráfico de barras empilhadas\n",
    "df = pd.pivot_table(index='vintage', columns='response', values='id', data=df3).reset_index()\n",
    "df.columns = ['vintage', 'no_response', 'yes_response']\n",
    "df.plot(x='vintage', kind='bar', stacked=True, ax=axes[2])\n",
    "axes[2].set_title('Gráfico de Barras Empilhadas')\n",
    "\n",
    "# Ajuste o layout dos gráficos\n",
    "plt.tight_layout()\n",
    "\n",
    "# Exiba os gráficos\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.10. Risk Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = df3[['risk_age','response']].groupby(['risk_age','response']).size().reset_index().rename(columns={0:'qtd'})\n",
    "ax = sns.barplot(x='risk_age',y='qtd',data=aux,hue='response');\n",
    "total = sum(aux['qtd'])\n",
    "graphic_percentage(ax,total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.11. Age Insured.\n",
    "INSIGHT:\n",
    "+ Dos que responderam que queria um seguro de automovél, 61,93% possuem idade entre 32 a 52 anos e não possuem seguro de veículo.\n",
    "+ Dos que possuem idade entre 32 a 52 anos e não possuem seguro de veículo, 61,1% são homens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = df3[['age_insured','response']].groupby(['age_insured','response']).size().reset_index().rename(columns={0:'qtd'})\n",
    "ax = sns.barplot(x='age_insured',y='qtd',data=aux,hue='response');\n",
    "total = sum(aux['qtd'])\n",
    "graphic_percentage(ax,total)\n",
    "\n",
    "correlation = df3['age_insured'].corr(df3['response'])\n",
    "print(f\"Correlação entre 'age_insured' e 'response': {correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux=aux[aux['response']==1]\n",
    "ax = sns.barplot(data=aux,x='age_insured',y='qtd');\n",
    "total=sum(aux['qtd'])\n",
    "\n",
    "# Adicione um insight abaixo do gráfico\n",
    "plt.text(0.5, -0.2, \"Insight: Das pessoas que responderam que queria um seguro, 61,93% possui idade entre 32 e 52 anos e não possuem seguro\",\n",
    "         ha='center', transform=plt.gca().transAxes)\n",
    "\n",
    "graphic_percentage(ax,total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra o DataFrame para pessoas com age_insured == 1\n",
    "filtered_df = df3[df3['age_insured'] == 1]\n",
    "\n",
    "# Calcula o número de homens com age_insured == 1\n",
    "homens_age_insured = len(filtered_df[filtered_df['gender'] == 'Male'])\n",
    "\n",
    "# Calcula o número de mulheres com age_insured == 1\n",
    "mulheres_age_insured = len(filtered_df[filtered_df['gender'] == 'Female'])\n",
    "\n",
    "# Define os rótulos e os valores para o gráfico de pizza\n",
    "labels = ['Homens', 'Mulheres']\n",
    "sizes = [homens_age_insured, mulheres_age_insured]\n",
    "colors = ['#ff9999', '#66b3ff']  # Cores para os setores do gráfico\n",
    "\n",
    "# Cria o gráfico de pizza\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, startangle=140)\n",
    "\n",
    "# Define o título do gráfico\n",
    "plt.title('Distribuição de Gênero entre Pessoas com age_insured == 1')\n",
    "\n",
    "# Exibe o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.12. Hyphoteses Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H1) Women buy more vehicles insurance than men\n",
    "False - Men (61,31%) have more interest in vehicles insurance than women (38,69%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = df3[df3['response']==1]\n",
    "aux = aux[['gender']].groupby('gender').size().reset_index().rename(columns={0:'qtd'})\n",
    "ax = sns.barplot(data=aux,x='gender',y='qtd')\n",
    "total = sum(aux['qtd'])\n",
    "graphic_percentage(ax,total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H2) Men tend to claim insurance more often\n",
    "True - Men (58,39%) have more damage vehicle than women (41,61%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = df3[df3['vehicle_damage']=='Yes']\n",
    "aux = aux[['gender']].groupby('gender').size().reset_index().rename(columns={0:'qtd'})\n",
    "ax = sns.barplot(data=aux,x='gender',y='qtd')\n",
    "total = sum(aux['qtd'])\n",
    "graphic_percentage(ax,total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H3) Customers without a driver's license are not interested in car insurance\n",
    "As the proportion of customers who do not have a wallet is very small in the dataset, customers who do not have a wallet will be analyzed individually.\n",
    "\n",
    "Paradox - As the base of those who do not have a license is small, and ~5% of them say they are interested in insurance. It could be an important group of consumers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux=df3[['driving_license','response']].groupby(['driving_license','response']).size().reset_index().rename(columns={0:'qtd'})\n",
    "ax=sns.barplot(data=aux,x='driving_license',y='qtd',hue='response');\n",
    "total=sum(aux['qtd'])\n",
    "graphic_percentage(ax,total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux=aux[aux['driving_license']==0]\n",
    "ax=sns.barplot(data=aux,x='driving_license',y='qtd',hue='response');\n",
    "total=sum(aux['qtd'])\n",
    "graphic_percentage(ax,total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H4) Customers over 30 are more interested in car insurance.\n",
    "True - Almost 85% of customer interesting on vehicle insurance has more then 30 years old\n",
    "\n",
    "INSIGHT: \n",
    "+ There is a group of 22 to 25 year olds who also have a reasonable interest. Despite being in the risk group.\n",
    "+ The most interested parties are between 33 and 54 years old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seu DataFrame auxiliar com a contagem\n",
    "aux = df3[['age', 'response']].groupby(['age', 'response']).size().reset_index().rename(columns={0: 'qtd'})\n",
    "\n",
    "# Crie uma paleta de cores com as cores desejadas para cada valor de 'response'\n",
    "palette = {0: 'blue', 1: 'red'}\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "ax = sns.barplot(data=aux, x='age', y='qtd', hue='response', palette=palette)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=8)\n",
    "plt.title('Contagem de Idade por Response')\n",
    "plt.xlabel('Idade')\n",
    "plt.ylabel('Quantidade')\n",
    "\n",
    "# Crie manualmente as legendas coloridas\n",
    "legend_labels = ['0 - Response', '1 - Response']\n",
    "colors = ['blue', 'red']\n",
    "handles = [plt.Rectangle((0, 0), 1, 1, color=colors[i], ec=\"k\") for i in range(len(legend_labels))]\n",
    "plt.legend(handles, legend_labels, title='Legend', loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = df3.copy()\n",
    "aux['age_category'] = df3['age'].apply(lambda x: 'until30' if x<30 else 'until60' if x>=30 and x<60 else 'more60')\n",
    "aux = aux[['age_category','response']].groupby(['age_category','response']).size().reset_index().rename(columns={0:'qtd'})\n",
    "ax = sns.barplot(data=aux,x='age_category',y='qtd',hue='response');\n",
    "total = sum(aux['qtd'])\n",
    "\n",
    "graphic_percentage(ax,total)\n",
    "\n",
    "\n",
    "aux=aux[aux['response']==1]\n",
    "ax=sns.barplot(data=aux,x='age_category',y='qtd');\n",
    "total=sum(aux['qtd'])\n",
    "\n",
    "graphic_percentage(ax,total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H5) Customers who have a car less than 2 years old are more interested in car insurance.\n",
    "True - Almost 89% of customer interesting on vehicle insurance has car age less then 2 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = df3[df3['response']==1]\n",
    "aux = aux[['vehicle_age']].groupby('vehicle_age').size().reset_index().rename(columns={0:'qtd'})\n",
    "ax = sns.barplot(data=aux,x='vehicle_age',y='qtd');\n",
    "total = sum(aux['qtd'])\n",
    "\n",
    "graphic_percentage(ax,total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H6) Customer that paid between 25000 a 40000 on Annual Premium have more interest in vehicles insurance\n",
    "True - Customer that paid between 25000 a 40000 on Annual Premium correspond a 53,40% of customer interesting in vehicles insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = df3.copy()\n",
    "aux['annual_premium_category'] = aux['annual_premium'].apply(lambda x: 'q1' if x<25000 else 'q4' if x>40000 else 'q2q3')\n",
    "aux=aux[['annual_premium_category','response']].groupby(['annual_premium_category','response']).size().reset_index().rename(columns={0:'qtd'})\n",
    "ax=sns.barplot(data=aux,x='annual_premium_category',y='qtd',hue='response');\n",
    "total=sum(aux['qtd'])\n",
    "graphic_percentage(ax,total)\n",
    "\n",
    "aux=aux[aux['response']==1]\n",
    "ax=sns.barplot(data=aux,x='annual_premium_category',y='qtd');\n",
    "total=sum(aux['qtd'])\n",
    "graphic_percentage(ax,total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.12. Summary of hypotheses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for the table\n",
    "tab = [['Hypothesis', 'Conclusion'],\n",
    "       ['H1', 'False'],\n",
    "       ['H2', 'True'],\n",
    "       ['H3', '-'],\n",
    "       ['H4', 'True'],\n",
    "       ['H5', 'True'],\n",
    "       ['H6', 'True']]\n",
    "\n",
    "# Print the table using the tabulate function from the tabulate library\n",
    "print(tabulate(tab, headers='firstrow'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. - Multivariate analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.1. - Numerical Attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix using Pearson correlation method for numeric attributes.\n",
    "correlation = numerical_columns.corr(method='pearson')\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(correlation, annot=True)\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.2. - Categorical Attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcule as correlações de Cramer's V para as variáveis categóricas\n",
    "matriz_corr_cramer_v = corr_cramer_v(categorical_columns)\n",
    "\n",
    "# Crie um DataFrame a partir da matriz de correlação\n",
    "df_corr_cramer_v = pd.DataFrame(matriz_corr_cramer_v, index=categorical_columns.columns, columns=categorical_columns.columns)\n",
    "\n",
    "# Defina o tamanho da figura\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "# Plote o mapa de calor\n",
    "sns.heatmap(df_corr_cramer_v, annot=True, cmap='coolwarm')\n",
    "\n",
    "# Exiba o mapa de calor\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0. - STEP SEVEN - DATA MODELING\n",
    "In this step, we will focus on preparing the data for training machine learning models. Data modeling is a crucial step in the model building process, as it ensures that the data is in a suitable format and contains the relevant information for learning the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame df3 it in df4.\n",
    "df4 = df3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting test dataset before \n",
    "x=df4.drop(columns='response')\n",
    "y=df4['response']\n",
    "\n",
    "# Redefina o índice do DataFrame x e y\n",
    "x.reset_index(drop=True, inplace=True)\n",
    "y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "split = ms.StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(x,y):\n",
    "    x_train,x_test = x.loc[train_index],x.loc[test_index]\n",
    "    y_train,y_test = y.loc[train_index],y.loc[test_index]\n",
    "    \n",
    "for train_index, test_index in split.split(x_train,y_train):\n",
    "    x_train,x_val = x_train.iloc[train_index],x.loc[test_index]\n",
    "    y_train,y_val = y_train.iloc[train_index],y.loc[test_index]\n",
    "\n",
    "df5=pd.concat([x_train,y_train],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. - Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pp.StandardScaler()\n",
    "\n",
    "# anual premium - StandarScaler\n",
    "df5['annual_premium'] = ss.fit_transform( df5[['annual_premium']].values )\n",
    "pickle.dump( ss, open( '../propensity_score/parameter/annual_premium_scaler.pkl', 'wb' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. - Rescaling:\n",
    "Rescaling, is another important data pre-processing step. Unlike normalization, scaling aims to place all variables on a specific scale, usually to facilitate interpretation or improve the performance of algorithms that are sensitive to the scale of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialize o MinMaxScaler\n",
    "mms_age = MinMaxScaler()\n",
    "# Aplique o MinMaxScaler à coluna \"age\"\n",
    "df5['age'] = mms_age.fit_transform(df5[['age']])\n",
    "# Salve o MinMaxScaler em um arquivo pickle\n",
    "pickle.dump(mms_age, open('../propensity_score/parameter/mms_age_scaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialize o MinMaxScaler\n",
    "mms_vintage = MinMaxScaler()\n",
    "# Vintage - MinMaxScaler\n",
    "df5['vintage'] = mms_vintage.fit_transform( df5[['vintage']].values )\n",
    "pickle.dump( mms_vintage, open( '../propensity_score/parameter/vintage_scaler.pkl', 'wb' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. - Transformation:\n",
    "Data transformation is an important step in data pre-processing, which aims to modify the distribution of variables to make them more suitable for statistical analysis or to improve the performance of machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1. - Encoding:\n",
    "Variable coding is an important data pre-processing step when dealing with categorical variables in our dataset. Categorical variables are those that represent different categories or classes, but do not have a natural order or hierarchy among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie um objeto LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "# Aplique o Label Encoding à coluna \"gender\"\n",
    "df5['gender'] = label_encoder.fit_transform(df5['gender'])\n",
    "# Salve o LabelEncoder em um arquivo pickle\n",
    "with open('../propensity_score/parameter/gender_label_encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(label_encoder, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie um objeto LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "# Aplique o Label Encoding à coluna \"vehicle_damage\"\n",
    "df5['vehicle_damage'] = label_encoder.fit_transform(df5['vehicle_damage'])\n",
    "# Salve o LabelEncoder em um arquivo pickle\n",
    "pickle.dump(label_encoder, open('../propensity_score/parameter/vehicle_damage_label_encoder.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # region_code - Target Encoding / Frequency Encoding\n",
    "# Calculando a taxa de resposta média para cada categoria em 'region_code'\n",
    "target_encode_region_code = df5.groupby('region_code')['response'].mean()\n",
    "\n",
    "# Mapeando os valores na coluna 'region_code' com as taxas de resposta média\n",
    "df5['region_code'] = df5['region_code'].map(target_encode_region_code)\n",
    "\n",
    "# Salve o dicionário target_encode_region_code em um arquivo pickle\n",
    "with open('../propensity_score/parameter/target_encode_region_code.pkl', 'wb') as file:\n",
    "    pickle.dump(target_encode_region_code, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy_sales_channel - Target Encoding / Frequency Encoding\n",
    "fe_policy_sales_channel = df5.groupby('policy_sales_channel').size() / len(df5)\n",
    "df5['policy_sales_channel'] = df5['policy_sales_channel'].map(fe_policy_sales_channel)\n",
    "pickle.dump(fe_policy_sales_channel, open('../propensity_score/parameter/fe_policy_sales_channel_scaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar colunas one-hot para a coluna \"vehicle_age\"\n",
    "def create_vehicle_age_columns(df5):\n",
    "    df5['below_1_year'] = df5['vehicle_age'].apply(lambda x: 1 if x == '< 1 Year' else 0)\n",
    "    df5['between_1_2_year'] = df5['vehicle_age'].apply(lambda x: 1 if x == '1-2 Year' else 0)\n",
    "    df5['over_2_years'] = df5['vehicle_age'].apply(lambda x: 1 if x == '> 2 Years' else 0)\n",
    "    df5.drop('vehicle_age', axis=1, inplace=True)  # Remover a coluna original\n",
    "\n",
    "# Chame a função para criar as colunas one-hot\n",
    "create_vehicle_age_columns(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2. - Response Variable Transformation:\n",
    "Transforming the response variable is an important step in data preparation when we are dealing with regression problems, where the response variable is continuous and we are looking to predict a numerical value. In this context, it is common that the distribution of the response variable is not ideal for applying certain regression algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.3. Nature Transformation:\n",
    "Nature transformation involves transforming variables based on their underlying nature or domain knowledge. This step is particularly useful when dealing with cyclic or periodic data, such as time series or circular data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5. - Selection of Variables:\n",
    "In this section, we will perform feature selection to choose the most relevant variables that will be used in our machine learning model. The goal is to remove any irrelevant or redundant features to improve the model's performance and reduce the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.1. - Boruta as Feature Selector:\n",
    "In this section, the Boruta algorithm is used as a feature selector. \n",
    "The Boruta algorithm is a feature selection method based on the Random Forest algorithm. It is used to identify the most important features in a dataset by comparing the importance of each feature with the importance of randomly created shadow features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_n = df5.drop( ['id', 'response'], axis=1 ).values\n",
    "y_train_n = y_train.values.ravel()\n",
    "\n",
    "# Define model\n",
    "et = en.ExtraTreesClassifier( n_jobs=-1 )\n",
    "\n",
    "# Define boruta\n",
    "boruta = bt.BorutaPy( et, n_estimators='auto', verbose=2, random_state=42 ).fit( x_train_n, y_train_n  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_selected = boruta.support_.tolist()\n",
    "\n",
    "# best features\n",
    "x_train_fs = df5.drop( ['id', 'response'], axis=1 )\n",
    "cols_selected_boruta = x_train_fs.iloc[:, cols_selected].columns.to_list()\n",
    "print('Colunas selecionadas pelo boruta:', cols_selected_boruta)\n",
    "\n",
    "# not selected boruta\n",
    "cols_not_selected_boruta = list( np.setdiff1d( x_train_fs.columns, cols_selected_boruta ) )\n",
    "print('Colunas não selecionadas pelo boruta:', cols_not_selected_boruta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.2. - Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "forest = en.ExtraTreesClassifier( n_estimators=250, random_state=0, n_jobs=-1 )\n",
    "\n",
    "# data preparation\n",
    "x_train_n = df5.drop( ['id', 'response'], axis=1 )\n",
    "y_train_n = y_train.values\n",
    "forest.fit( x_train_n, y_train_n )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "df = pd.DataFrame()\n",
    "for i, j in zip( x_train_n, forest.feature_importances_ ):\n",
    "    aux = pd.DataFrame( {'feature': i, 'importance': j}, index=[0] )\n",
    "    df = pd.concat( [df, aux], axis=0 )\n",
    "    \n",
    "print( df.sort_values( 'importance', ascending=False ) )\n",
    "\n",
    "# Plot the impurity-based feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(x_train_n.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(x_train_n.shape[1]), indices)\n",
    "plt.xlim([-1, x_train_n.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6. - Manual Feature Selection:\n",
    "The manual selection of features is the selection of the most important columns, defined by boruta, so we don't need to run it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_selected = ['annual_premium', 'vintage', 'age', 'region_code', 'vehicle_damage','policy_sales_channel',  'previously_insured', 'age_insured']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0. - STEP EIGHT - TRAINING OF MACHINE LEARNING ALGORITHMS\n",
    "In this step we will train the machine learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar transformações em x_validation\n",
    "x_validation = apply_transformations(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de Treinamento\n",
    "x_train = df5[cols_selected]\n",
    "y_train = y_train  # Mantenha a variável de destino\n",
    "\n",
    "# Dados de Validação\n",
    "x_val = x_validation[cols_selected]\n",
    "y_val = y_val  # Mantenha a variável de destino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. - KNN Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma instância do modelo KNeighborsClassifier\n",
    "knn_model = KNeighborsClassifier(n_neighbors=7)  # Você pode ajustar o valor de 'n_neighbors' conforme necessário\n",
    "\n",
    "# Treinar o modelo nos dados de treinamento\n",
    "knn_model.fit(x_train, y_train)\n",
    "\n",
    "# Fazer previsões de classe nos dados de validação\n",
    "y_val_pred_knn = knn_model.predict(x_val)\n",
    "\n",
    "# Fazer previsões de probabilidade nos dados de validação\n",
    "y_val_prob_knn = knn_model.predict_proba(x_val)\n",
    "\n",
    "# 'y_val_pred' conterá as classes previstas (0 ou 1) para cada amostra nos dados de validação\n",
    "# 'y_val_prob' conterá as probabilidades previstas para cada classe (0 e 1) para cada amostra nos dados de validação\n",
    "\n",
    "# Crie uma figura com dois subplots lado a lado\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "# Plot da Curva de Ganho Acumulado no primeiro subplot\n",
    "skplt.metrics.plot_cumulative_gain(y_val, y_val_prob_knn, ax=ax1)\n",
    "ax1.set_title(\"Curva de Ganho Acumulado\")\n",
    "\n",
    "# Plot da Curva de Lift no segundo subplot\n",
    "skplt.metrics.plot_lift_curve(y_val, y_val_prob_knn, ax=ax2)\n",
    "ax2.set_title(\"Curva de Lift\")\n",
    "\n",
    "# Ajuste o layout e mostre os gráficos\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = knn_model \n",
    "classification_metrics = calculate_classification_and_ranking_metrics(model, x_val, y_val, y_val_prob=y_val_prob_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1 - Cross Validation - KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model_cv = cross_validate(knn_model, x_train, y_train, num_folds=5)\n",
    "knn_model_cv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma instância do modelo LogisticRegression\n",
    "logistic_model = LogisticRegression( random_state=42)\n",
    "\n",
    "# Treinar o modelo nos dados de treinamento\n",
    "logistic_model.fit(x_train, y_train)\n",
    "\n",
    "# Fazer previsões de classe nos dados de validação\n",
    "y_val_pred_logistic = logistic_model.predict(x_val)\n",
    "\n",
    "# Fazer previsões de probabilidade nos dados de validação\n",
    "y_val_prob_logistic = logistic_model.predict_proba(x_val)\n",
    "\n",
    "# 'y_val_pred_logistic' conterá as classes previstas (0 ou 1) para cada amostra nos dados de validação\n",
    "# 'y_val_prob_logistic' conterá as probabilidades previstas para cada classe (0 e 1) para cada amostra nos dados de validação\n",
    "\n",
    "# Crie uma figura com dois subplots lado a lado\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "# Plot da Curva de Ganho Acumulado no primeiro subplot\n",
    "skplt.metrics.plot_cumulative_gain(y_val, y_val_prob_logistic, ax=ax1)\n",
    "ax1.set_title(\"Curva de Ganho Acumulado\")\n",
    "\n",
    "# Plot da Curva de Lift no segundo subplot\n",
    "skplt.metrics.plot_lift_curve(y_val, y_val_prob_logistic, ax=ax2)\n",
    "ax2.set_title(\"Curva de Lift\")\n",
    "\n",
    "# Ajuste o layout e mostre os gráficos\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = logistic_model \n",
    "classification_metrics = calculate_classification_and_ranking_metrics(model, x_val, y_val, y_val_prob=y_val_prob_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1 - Cross Validation - Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model_cv = cross_validate(logistic_model, x_train, y_train, num_folds=5)\n",
    "logistic_model_cv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. LGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model = LGBMClassifier(force_row_wise=True, verbose=-1) # Criar uma instância do modelo LGBMClassifier\n",
    "\n",
    "# Treinar o modelo nos dados de treinamento\n",
    "lgbm_model.fit(x_train, y_train)\n",
    "\n",
    "# Fazer previsões de classe nos dados de validação\n",
    "y_val_pred_lgbm = lgbm_model.predict(x_val)\n",
    "\n",
    "# Fazer previsões de probabilidade nos dados de validação (se necessário)\n",
    "y_val_prob_lgbm = lgbm_model.predict_proba(x_val)\n",
    "\n",
    "# Crie uma figura com dois subplots lado a lado\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "# Plot da Curva de Ganho Acumulado no primeiro subplot\n",
    "skplt.metrics.plot_cumulative_gain(y_val, y_val_prob_lgbm, ax=ax1)\n",
    "ax1.set_title(\"Curva de Ganho Acumulado\")\n",
    "\n",
    "# Plot da Curva de Lift no segundo subplot\n",
    "skplt.metrics.plot_lift_curve(y_val, y_val_prob_lgbm, ax=ax2)\n",
    "ax2.set_title(\"Curva de Lift\")\n",
    "\n",
    "# Ajuste o layout e mostre os gráficos\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgbm_model \n",
    "classification_metrics = calculate_classification_and_ranking_metrics(model, x_val, y_val, y_val_prob=y_val_prob_lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1 - Cross Validation - LGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model_cv = cross_validate(lgbm_model, x_train, y_train, num_folds=5)\n",
    "lgbm_model_cv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4. Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier( n_estimators=1000, n_jobs=-1, random_state=42 ).fit(x_train, y_train)\n",
    "\n",
    "# Fazer previsões de classe nos dados de validação\n",
    "y_val_pred_rf = rf_model.predict(x_val)\n",
    "\n",
    "# Fazer previsões de probabilidade nos dados de validação (se necessário)\n",
    "y_val_prob_rf = rf_model.predict_proba(x_val)\n",
    "\n",
    "# Crie uma figura com dois subplots lado a lado\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "# Plot da Curva de Ganho Acumulado no primeiro subplot\n",
    "skplt.metrics.plot_cumulative_gain(y_val, y_val_prob_rf, ax=ax1)\n",
    "ax1.set_title(\"Curva de Ganho Acumulado\")\n",
    "\n",
    "# Plot da Curva de Lift no segundo subplot\n",
    "skplt.metrics.plot_lift_curve(y_val, y_val_prob_rf, ax=ax2)\n",
    "ax2.set_title(\"Curva de Lift\")\n",
    "\n",
    "# Ajuste o layout e mostre os gráficos\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rf_model \n",
    "classification_metrics = calculate_classification_and_ranking_metrics(rf_model, x_val, y_val, y_val_prob=y_val_prob_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.1 - Cross Validation - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_cv = cross_validate(rf_model, x_train, y_train, num_folds=5)\n",
    "rf_model_cv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5. XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma instância do modelo XGBClassifier\n",
    "xgb_model = XGBClassifier(\n",
    "    objective='binary:logistic',  # Para problemas de classificação binária\n",
    "    eval_metric='auc',  # Métrica de avaliação (ROC AUC)\n",
    "    n_estimators=100,  # Número de árvores (pode ajustar conforme necessário)\n",
    "    max_depth=6,  # Profundidade máxima da árvore (pode ajustar conforme necessário)\n",
    "    learning_rate=0.1,  # Taxa de aprendizado (pode ajustar conforme necessário)\n",
    "    random_state=42  # Seed para reprodutibilidade\n",
    ")\n",
    "\n",
    "# Treinar o modelo nos dados de treinamento\n",
    "xgb_model.fit(x_train, y_train)\n",
    "\n",
    "# Fazer previsões de classe nos dados de validação\n",
    "y_val_pred_xgb = xgb_model.predict(x_val)\n",
    "\n",
    "# Fazer previsões de probabilidade nos dados de validação\n",
    "y_val_prob_xgb = xgb_model.predict_proba(x_val)\n",
    "\n",
    "# Crie uma figura com dois subplots lado a lado\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "# Plot da Curva de Ganho Acumulado no primeiro subplot\n",
    "skplt.metrics.plot_cumulative_gain(y_val, y_val_prob_xgb, ax=ax1)\n",
    "ax1.set_title(\"Curva de Ganho Acumulado\")\n",
    "\n",
    "# Plot da Curva de Lift no segundo subplot\n",
    "skplt.metrics.plot_lift_curve(y_val, y_val_prob_xgb, ax=ax2)\n",
    "ax2.set_title(\"Curva de Lift\")\n",
    "\n",
    "# Ajuste o layout e mostre os gráficos\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb_model\n",
    "classification_metrics = calculate_classification_and_ranking_metrics(xgb_model, x_val, y_val, y_val_prob=y_val_prob_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.1 - Cross Validation - XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_cv = cross_validate(xgb_model, x_train, y_train, num_folds=5)\n",
    "xgb_model_cv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6. Performance Metrics Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialize os modelos\n",
    "models = [\n",
    "    (\"KNN Classifier\", KNeighborsClassifier(n_neighbors=7)),\n",
    "    (\"Logistic Regression\", LogisticRegression()),\n",
    "    (\"LGBM Classifier\", LGBMClassifier(force_row_wise=True, verbose=-1)),\n",
    "    (\"Random Forest Classifier\", RandomForestClassifier( n_estimators=1000, n_jobs=-1, random_state=42 )),\n",
    "    (\"XGBoost Classifier\", XGBClassifier(objective='binary:logistic', eval_metric='auc', n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42))\n",
    "]\n",
    "\n",
    "# Inicialize um DataFrame para armazenar as métricas\n",
    "metrics_df = pd.DataFrame(columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"AUC-ROC\"])\n",
    "\n",
    "# Loop através dos modelos\n",
    "for model_name, model in models:\n",
    "    # Treine o modelo\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    # Faça previsões no conjunto de validação\n",
    "    y_pred = model.predict(x_val)\n",
    "    y_prob = model.predict_proba(x_val)[:, 1]  # Probabilidade da classe positiva\n",
    "    \n",
    "    # Calcule as métricas\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    auc_roc = roc_auc_score(y_val, y_prob)\n",
    "    \n",
    "    # Adicione as métricas ao DataFrame\n",
    "    metrics_df.loc[len(metrics_df)] = [model_name, accuracy, precision, recall, f1, auc_roc]\n",
    "\n",
    "# Exiba a tabela comparativa\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6.1 Cross Validation Performance Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina o número de folds (k)\n",
    "num_folds = 5\n",
    "\n",
    "# Crie um objeto StratifiedKFold para estratificar os folds com base nas classes\n",
    "stratified_kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Lista para armazenar as métricas de desempenho para cada modelo\n",
    "model_metrics = []\n",
    "\n",
    "# Lista de modelos treinados (substitua com seus modelos)\n",
    "trained_models = [knn_model, logistic_model, lgbm_model, rf_model, xgb_model]\n",
    "\n",
    "# Métricas que você deseja calcular\n",
    "metrics = {\n",
    "    'Accuracy': accuracy_score,\n",
    "    'Precision': precision_score,\n",
    "    'Recall': recall_score,\n",
    "    'F1 Score': f1_score,\n",
    "    'AUC-ROC': roc_auc_score\n",
    "}\n",
    "\n",
    "# Loop através de cada modelo treinado\n",
    "for model in trained_models:\n",
    "    model_name = model.__class__.__name__\n",
    "    \n",
    "    # Execute a validação cruzada para o modelo atual\n",
    "    y_pred = cross_val_predict(model, x_train, y_train, cv=stratified_kfold, method='predict_proba')\n",
    "    y_prob = y_pred[:, 1]  # Probabilidades da classe positiva\n",
    "    \n",
    "    # Calcule as métricas de desempenho para cada fold\n",
    "    fold_metrics = {}\n",
    "    for metric_name, metric_func in metrics.items():\n",
    "        if metric_name == 'AUC-ROC':\n",
    "            metric_value = metric_func(y_train, y_prob)\n",
    "        else:\n",
    "            metric_value = metric_func(y_train, (y_prob > 0.5).astype(int))\n",
    "        fold_metrics[metric_name] = metric_value\n",
    "    \n",
    "    # Adicione as métricas ao modelo atual na lista\n",
    "    model_metrics.append({\n",
    "        'Model': model_name,\n",
    "        **fold_metrics\n",
    "    })\n",
    "\n",
    "# Crie um DataFrame com as métricas de desempenho\n",
    "model_metrics_df = pd.DataFrame(model_metrics)\n",
    "\n",
    "# Exiba as métricas de desempenho\n",
    "model_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.0. - STEP NINE - FINE TUNING\n",
    "In this step, we focus on improving the models' performance by fine-tuning their hyperparameters. This process involves searching for the best combination of hyperparameters that optimize the models' performance on the training data. Fine-tuning is essential to achieve better accuracy and generalization on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. - Random Search:\n",
    "Random Search, we will perform hyperparameter tuning using the Random Search technique. Random Search is a hyperparameter optimization method that randomly samples hyperparameter combinations from a predefined search space. It is an efficient way to explore a wide range of hyperparameter values without trying all possible combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1 - Random Search: Fine-Tuning para Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defina a grade de hiperparâmetros que deseja pesquisar\n",
    "# param_grid = {\n",
    "#     'C': loguniform(1e-6, 1e+6),  # Parâmetro de regularização\n",
    "#     'penalty': ['l1', 'l2'],  # Tipo de regularização\n",
    "# }\n",
    "\n",
    "# # Crie uma instância do modelo Logistic Regression\n",
    "# logistic_regression = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# # Realize a pesquisa aleatória\n",
    "# logistic_search = RandomizedSearchCV(\n",
    "#     logistic_regression, param_distributions=param_grid,\n",
    "#     n_iter=5,  # Número de iterações\n",
    "#     scoring='roc_auc',  # Métrica de avaliação\n",
    "#     cv=5,  # Número de dobras para validação cruzada\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Execute a pesquisa aleatória nos dados de treinamento\n",
    "# logistic_search.fit(x_train, y_train)\n",
    "\n",
    "# # Exiba os melhores hiperparâmetros encontrados\n",
    "# print(\"Melhores Hiperparâmetros para Logistic Regression:\", logistic_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.2 - Random Search: Fine-Tuning para LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defina a grade de hiperparâmetros que deseja pesquisar\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300],  # Número de árvores\n",
    "#     'learning_rate': uniform(0.01, 0.1),  # Taxa de aprendizado\n",
    "#     'max_depth': [5, 10, 15],  # Profundidade máxima da árvore\n",
    "# }\n",
    "\n",
    "# # Crie uma instância do modelo LightGBM\n",
    "# lgbm_model = LGBMClassifier(random_state=42, force_row_wise=True, verbose=-1)\n",
    "\n",
    "# # Realize a pesquisa aleatória\n",
    "# lgbm_search = RandomizedSearchCV(\n",
    "#     lgbm_model, param_distributions=param_grid,\n",
    "#     n_iter=5,  # Número de iterações\n",
    "#     scoring='roc_auc',  # Métrica de avaliação\n",
    "#     cv=5,  # Número de dobras para validação cruzada\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Execute a pesquisa aleatória nos dados de treinamento\n",
    "# lgbm_search.fit(x_train, y_train)\n",
    "\n",
    "# # Exiba os melhores hiperparâmetros encontrados\n",
    "# print(\"Melhores Hiperparâmetros para LightGBM:\", lgbm_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.3 - Random Search: Fine-Tuning para XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defina a grade de hiperparâmetros que deseja pesquisar\n",
    "# param_grid = {\n",
    "#     'n_estimators': [80, 100, 150, 200, 500, 1500, 2500, 3500],\n",
    "#     'eta': [0.01, 0.03],\n",
    "#     'max_depth': [3, 5, 9],\n",
    "#     'subsample': [0.1, 0.5, 0.7],\n",
    "#     'colsample_bytree': [0.3, 0.7, 0.9]\n",
    "# }\n",
    "\n",
    "# # Crie uma instância do modelo XGBoost Classifier\n",
    "# xgb_model = XGBClassifier(objective='binary:logistic', eval_metric='logloss', random_state=42)\n",
    "\n",
    "# # Realize a pesquisa aleatória\n",
    "# xgb_search = RandomizedSearchCV(\n",
    "#     xgb_model, param_distributions=param_grid,\n",
    "#     n_iter=5,  # Número de iterações\n",
    "#     scoring='roc_auc',  # Métrica de avaliação\n",
    "#     cv=5,  # Número de dobras para validação cruzada\n",
    "#     verbose=2,  # Exiba informações detalhadas durante a busca\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1  # Use todos os núcleos da CPU para acelerar a busca\n",
    "# )\n",
    "\n",
    "# # Execute a pesquisa aleatória nos dados de treinamento\n",
    "# xgb_search.fit(x_train, y_train)\n",
    "\n",
    "# # Exiba os melhores hiperparâmetros encontrados\n",
    "# print(\"Melhores Hiperparâmetros para XGBoost Classifier:\", xgb_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.4 - Random Search: Fine-Tuning para Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defina a grade de hiperparâmetros que deseja pesquisar\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300, 500, 1000],  # Número de árvores na floresta\n",
    "#     'max_depth': [None, 10, 20, 30],  # Profundidade máxima da árvore\n",
    "#     'min_samples_split': [2, 5, 10],  # Número mínimo de amostras para dividir um nó\n",
    "#     'min_samples_leaf': [1, 2, 4],  # Número mínimo de amostras em uma folha\n",
    "#     'max_features': ['auto', 'sqrt', 'log2']  # Número máximo de recursos a considerar em cada divisão\n",
    "# }\n",
    "\n",
    "# # Crie uma instância do modelo Random Forest Classifier\n",
    "# rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# # Realize a pesquisa aleatória\n",
    "# rf_search = RandomizedSearchCV(\n",
    "#     rf_model, param_distributions=param_grid,\n",
    "#     n_iter=5,  # Número de iterações\n",
    "#     scoring='roc_auc',  # Métrica de avaliação\n",
    "#     cv=5,  # Número de dobras para validação cruzada\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Execute a pesquisa aleatória nos dados de treinamento\n",
    "# rf_search.fit(x_train, y_train)\n",
    "\n",
    "# # Exiba os melhores hiperparâmetros encontrados\n",
    "# print(\"Melhores Hiperparâmetros para Random Forest Classifier:\", rf_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.5 - Random Search: Fine-Tuning para K-Nearest Neighbors (KNN) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defina a grade de hiperparâmetros que deseja pesquisar\n",
    "# param_grid = {\n",
    "#     'n_neighbors': [3, 5, 7],  # Número de vizinhos\n",
    "#     'weights': ['uniform', 'distance'],  # Peso dos vizinhos\n",
    "#     'p': [1, 2],  # Parâmetro para a distância de Minkowski (1 para Manhattan, 2 para Euclidiana)\n",
    "# }\n",
    "\n",
    "# # Crie uma instância do modelo K-Nearest Neighbors (KNN) Classifier\n",
    "# knn_model = KNeighborsClassifier()\n",
    "\n",
    "# # Realize a pesquisa aleatória\n",
    "# knn_search = RandomizedSearchCV(\n",
    "#     knn_model, param_distributions=param_grid,\n",
    "#     n_iter=5,  # Número de iterações\n",
    "#     scoring='roc_auc',  # Métrica de avaliação\n",
    "#     cv=5,  # Número de dobras para validação cruzada\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Execute a pesquisa aleatória nos dados de treinamento\n",
    "# knn_search.fit(x_train, y_train)\n",
    "\n",
    "# # Exiba os melhores hiperparâmetros encontrados\n",
    "# print(\"Melhores Hiperparâmetros para K-Nearest Neighbors (KNN) Classifier:\", knn_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. - Final Parameters Adopted for Random Forest Classifier:\n",
    "In this section, you should present the final set of hyperparameters that were selected for the Random Forest Classifier model based on the results of the random search or any other hyperparameter tuning method used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Razões para uso do RandomForestClassifier:\n",
    "\n",
    "O modelo performou bem com as métricas padrão e, como o objetivo é criar uma lista de clientes mais propensos a comprar o seguro, o modelo que parece ser mais apropriado é o \"RandomForestClassifier\":\n",
    "\n",
    "**Classification Metrics (Métricas de Classificação):**\n",
    "1. **Accuracy (Acurácia)**: Mede a taxa geral de acerto do modelo, ou seja, a proporção de previsões corretas em relação ao total de previsões. Neste caso, a acurácia é de aproximadamente 95,10%, indicando que o modelo acerta a classificação em cerca de 95,10% das vezes.\n",
    "\n",
    "2. **Balanced Accuracy (Acurácia Balanceada)**: Leva em consideração o desequilíbrio de classes no conjunto de dados. É útil quando as classes estão desproporcionalmente distribuídas. Neste caso, a acurácia balanceada é de aproximadamente 83,35%, sugerindo que o modelo tem um bom desempenho, considerando o desequilíbrio de classes.\n",
    "\n",
    "3. **Kappa Score (Índice Kappa)**: Compara o desempenho do modelo com o desempenho esperado ao acaso. Leva em conta o equilíbrio entre previsões corretas e previsões ao acaso. Neste caso, o índice Kappa é de aproximadamente 0,74, indicando um bom desempenho em relação ao acaso.\n",
    "\n",
    "4. **Precision Score (Precisão)**: Mede a proporção de previsões positivas corretas em relação ao total de previsões positivas feitas pelo modelo. Neste caso, a precisão é de aproximadamente 89,20%, o que indica que, das previsões positivas do modelo, cerca de 89,20% estão corretas.\n",
    "\n",
    "5. **Recall Score (Revocação ou Sensibilidade)**: Mede a proporção de instâncias positivas corretamente previstas em relação ao total de instâncias positivas no conjunto de dados. Neste caso, a revocação é de aproximadamente 67,84%, indicando que o modelo identificou corretamente cerca de 67,84% das instâncias positivas.\n",
    "\n",
    "6. **F1 Score**: É a média harmônica entre precisão e revocação. É útil quando se deseja equilibrar precisão e revocação. Neste caso, o F1 Score é de aproximadamente 0,77, sugerindo um bom equilíbrio entre precisão e revocação.\n",
    "\n",
    "7. **Matthew Correlation Score (Coeficiente de Correlação de Matthews)**: É uma medida do desempenho geral do modelo, considerando verdadeiros positivos, verdadeiros negativos, falsos positivos e falsos negativos. Neste caso, o coeficiente de correlação de Matthews é de aproximadamente 0,75, indicando um bom desempenho geral.\n",
    "\n",
    "**Ranking Metrics (Métricas de Classificação por Ranking):**\n",
    "1. **ROC AUC**: A Área sob a Curva ROC (Receiver Operating Characteristic) é uma métrica que avalia a capacidade do modelo de distinguir entre classes positivas e negativas. Neste caso, a ROC AUC é de aproximadamente 83,35%, indicando um bom desempenho na diferenciação entre as classes.\n",
    "\n",
    "2. **Top K Score**: Mede o desempenho do modelo em relação aos K principais resultados. Neste caso, o Top K Score é de aproximadamente 95,10%, indicando que o modelo tem um bom desempenho na classificação das principais previsões.\n",
    "\n",
    "Em resumo, as métricas indicam que o modelo tem um desempenho geral muito bom, com alta precisão e revocação, além de uma boa capacidade de diferenciação entre classes. O índice Kappa também sugere que o desempenho é significativamente melhor do que o esperado ao acaso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo final com os melhores hiperparâmetros\n",
    "rf_model_bp = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Treine o modelo final com todos os dados de treinamento (treinamento + validação)\n",
    "x_train_final = pd.concat([x_train, x_val])\n",
    "y_train_final = pd.concat([y_train, y_val])\n",
    "\n",
    "rf_model_bp.fit(x_train_final, y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Salve o dicionário do modelo em um arquivo pickle\n",
    "# with open('../propensity_score/rf_model_bp.pkl', 'wb') as file:\n",
    "#     pickle.dump(rf_model_bp, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.0. - STEP TEN - TRANSLATION AND INTERPRETATION OF THE ERROR FOR THE BUSINESS TEAM\n",
    "In this step, the goal is to translate and interpret the error metrics calculated during the evaluation of the models for the business team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando as transformações no conjunto de teste\n",
    "x_testing = apply_transformations(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_testing[cols_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer previsões de classe nos dados de teste\n",
    "y_test_pred_rf = rf_model_bp.predict(x_test)\n",
    "\n",
    "# Fazer previsões de probabilidade nos dados de teste (se necessário)\n",
    "y_test_prob_rf = rf_model_bp.predict_proba(x_test)\n",
    "\n",
    "# Avalie o desempenho do modelo final nos dados de teste\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_test_pred_rf)\n",
    "precision = precision_score(y_test, y_test_pred_rf)\n",
    "recall = recall_score(y_test, y_test_pred_rf)\n",
    "f1 = f1_score(y_test, y_test_pred_rf)\n",
    "roc_auc = roc_auc_score(y_test, y_test_prob_rf[:, 1])  # Use as probabilidades da classe positiva\n",
    "\n",
    "print(\"Desempenho do Modelo Final nos Dados de Teste:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie uma figura com dois subplots lado a lado\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "# Plot da Curva de Ganho Acumulado no primeiro subplot\n",
    "skplt.metrics.plot_cumulative_gain(y_test, y_test_prob_rf, ax=ax1)\n",
    "ax1.set_title(\"Curva de Ganho Acumulado\")\n",
    "\n",
    "# Plot da Curva de Lift no segundo subplot\n",
    "skplt.metrics.plot_lift_curve(y_test, y_test_prob_rf, ax=ax2)\n",
    "ax2.set_title(\"Curva de Lift\")\n",
    "\n",
    "# Ajuste o layout e mostre os gráficos\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie um novo DataFrame com as probabilidades previstas e os rótulos reais nos dados de teste\n",
    "df_results = x_test.copy()\n",
    "df_results['response'] = y_test  # Adicione os rótulos reais\n",
    "df_results['predicted_prob'] = y_test_prob_rf[:, 1]  # Use as probabilidades previstas do modelo final\n",
    "\n",
    "# Ordene o DataFrame por probabilidades previstas em ordem decrescente\n",
    "df_results = df_results.sort_values(by='predicted_prob', ascending=False)\n",
    "\n",
    "# Determine o valor de K como 50% dos clientes com maiores probabilidades\n",
    "K = int(0.50 * len(df_results))\n",
    "\n",
    "# Selecione os top-K clientes\n",
    "top_K_customers = df_results.head(K)\n",
    "\n",
    "# Calcule a precisão no top-K\n",
    "precision = top_K_customers['response'].sum() / K\n",
    "\n",
    "# Calcule o recall no top-K\n",
    "total_actual_purchases = df_results['response'].sum()\n",
    "recall = top_K_customers['response'].sum() / total_actual_purchases\n",
    "\n",
    "print(f'Precision at K: {precision:.4f}')\n",
    "print(f'Recall at K: {recall:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As métricas Precision at K e Recall at K revelam a eficácia do modelo na identificação de clientes propensos a comprar seguro de automóvel. A Precision at K indica que aproximadamente 23,99% dos K clientes de maior probabilidade realmente adquirem o seguro quando contatados pelo time de vendas. Enquanto isso, o Recall at K é notavelmente alto em 0,9936, indicando que a maioria dos compradores reais está entre os top-K clientes com maiores probabilidades previstas. Embora o modelo seja eficaz em identificar compradores em potencial, a precisão pode ser aprimorada, sugerindo a oportunidade de refinar a seleção de clientes a serem contatados e, assim, otimizar a conversão de vendas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1. - Evaluating Model Overestimation or Underestimation of the final model in Test:\n",
    "In this section, the code aims to evaluate whether the final model tends to overestimate or underestimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular a diferença entre as probabilidades previstas e a taxa real de compra do seguro\n",
    "df_results['prob_diff'] = df_results['predicted_prob'] - df_results['response']\n",
    "\n",
    "# Determinar se o modelo está superestimando ou subestimando\n",
    "df_results['estimation'] = np.where(df_results['prob_diff'] > 0, 'Superestimando', 'Subestimando')\n",
    "\n",
    "# Contar quantas vezes o modelo superestimou e subestimou\n",
    "estimation_counts = df_results['estimation'].value_counts()\n",
    "\n",
    "# Exibir os resultados\n",
    "print(\"Contagem de Superestimação e Subestimação:\")\n",
    "print(estimation_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo subestimou cerca de 33.795 vezes e superestimou cerca de 29.295 vezes. A diferença entre essas contagens indica que o modelo está inclinado a fazer previsões mais conservadoras, subestimando com mais frequência do que superestimando.\n",
    "\n",
    "Isso significa que o modelo previu uma probabilidade menor de compra do seguro do que a taxa real de compra do seguro. Em outras palavras, o modelo tende a ser mais conservador em suas previsões, prevendo menos compras do que realmente acontecem. É preferível subestimar a probabilidade de um evento para evitar ações desnecessárias ou custos adicionais para a equipe de marketing.\n",
    "\n",
    "Dada a pequena diferença entre ambas as contagens e o problema de negócio enfrentado, pode-se considerar que o modelo está se saindo bem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2. - Respondendo as pergunta de negócio:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.1. - Fazendo upload do conjunto de teste e determinando o melhor threshold para o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv( '../propensity_score/data/raw/test.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = apply_transformations(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = feature_engineering(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[cols_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores de threshold que você deseja testar\n",
    "thresholds = [0.1, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# Dados reais e probabilidades previstas\n",
    "y_true = df_results['response']\n",
    "predicted_probs = df_results['predicted_prob']\n",
    "\n",
    "# Inicialize as listas para armazenar as métricas\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "roc_aucs = []\n",
    "\n",
    "# Itere sobre os diferentes valores de threshold\n",
    "for threshold in thresholds:\n",
    "    # Converte as probabilidades em previsões binárias com base no threshold\n",
    "    y_pred = (predicted_probs >= threshold).astype(int)\n",
    "    \n",
    "    # Calcule as métricas\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, predicted_probs)\n",
    "    \n",
    "    # Armazene as métricas nas listas\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    roc_aucs.append(roc_auc)\n",
    "\n",
    "# Crie um DataFrame para visualizar as métricas para cada threshold\n",
    "results_df = pd.DataFrame({'Threshold': thresholds,\n",
    "                           'Precision': precisions,\n",
    "                           'Recall': recalls,\n",
    "                           'F1 Score': f1_scores,\n",
    "                           'ROC AUC': roc_aucs})\n",
    "\n",
    "# Encontre o threshold que produziu o melhor F1 Score\n",
    "best_f1_threshold = results_df.loc[results_df['F1 Score'].idxmax()]\n",
    "\n",
    "print(\"Resultados da Busca de Thresholds:\")\n",
    "print(results_df)\n",
    "\n",
    "print(\"\\nMelhor Threshold com Base no F1 Score:\")\n",
    "print(best_f1_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.2. - Principais insights sobre os atributos mais relevantes de clientes interessados em adquirir um seguro de automóvel:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Clientes com idades entre 30 e 50 anos têm mais interesse em seguro de veículos.\n",
    "\n",
    "+ Clientes com carros mais novos têm mais interesse em seguro de veículos.\n",
    "\n",
    "+ Clientes em regiões específicas têm mais interesse em seguro de veículos.\n",
    "\n",
    "+ Os atributos mais relevantes para definir o interesse do cliente são (previouslyInsured, annualPremium, vintage, age, regionCode, policySalesChannel, vehicleDamage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.3. - Porcentagem de clientes interessados se o time de vendas conseguir fazer 40.000 ligações:\n",
    "Ao contatar os clientes com as maiores probabilidades previstas de compra de seguro (usando um threshold de 0.25), o time de vendas conseguirá alcançar aproximadamente 81.79% dos clientes que são considerados interessados em adquirir um seguro de automóvel de acordo com o modelo. Em outras palavras, ao fazer 40.000 ligações para os clientes mais propensos, cerca de 81.79% deles têm uma probabilidade prevista de compra de seguro igual ou maior que 0.25, indicando um alto potencial de conversão. Isso demonstra a eficácia do direcionamento com base nas previsões do modelo para maximizar as chances de sucesso nas vendas. Pelo menos 2,5 vezes mais do que o método aleatório."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faça previsões de probabilidade no conjunto de testes\n",
    "y_test_prob_rf = rf_model_bp.predict_proba(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calls = 40000\n",
    "total_leads = df_test.shape[0]\n",
    "percent_calls = calls / total_leads\n",
    "print('Insurance All total leads: {}'.format(total_leads))\n",
    "print('Insurance All % of calls: {0:.2f}%'.format(percent_calls*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina o número de ligações\n",
    "num_ligacoes = 40000\n",
    "\n",
    "# Selecione aleatoriamente K clientes para contatar com base nas probabilidades previstas\n",
    "# Certifique-se de que K seja menor ou igual ao tamanho do conjunto de testes\n",
    "K = min(num_ligacoes, len(df_test))\n",
    "\n",
    "# Ordene o DataFrame de testes com base nas probabilidades previstas em ordem decrescente\n",
    "df_test_results = df_test.copy()\n",
    "df_test_results['predicted_prob'] = y_test_prob_rf[:, 1]\n",
    "df_test_results = df_test_results.sort_values(by='predicted_prob', ascending=False)\n",
    "\n",
    "# Selecione os top-K clientes para contatar\n",
    "top_K_to_contact = df_test_results.head(K)\n",
    "\n",
    "# Calcule a porcentagem de clientes interessados entre os contatados\n",
    "porcentagem_interessados_contatados = (top_K_to_contact['predicted_prob'] >= 0.25).mean() * 100\n",
    "\n",
    "print(f'Porcentagem de Clientes Interessados Contatados: {porcentagem_interessados_contatados:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.4. - Quantas ligações o time de vendas precisa fazer para contatar 80% dos clientes interessados em adquirir um seguro de automóvel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina a porcentagem desejada de clientes interessados a serem contatados\n",
    "porcentagem_desejada = 80  # Porcentagem desejada (80%)\n",
    "\n",
    "# Ordene o DataFrame de teste com base nas probabilidades previstas em ordem decrescente\n",
    "df_test_results = df_test.copy()\n",
    "df_test_results['predicted_prob'] = y_test_prob_rf[:, 1]\n",
    "df_test_results = df_test_results.sort_values(by='predicted_prob', ascending=False)\n",
    "\n",
    "# Calcule o número total de clientes interessados no conjunto de teste (threshold >= 0.25)\n",
    "total_clientes_interessados = (df_test_results['predicted_prob'] >= 0.25).sum()\n",
    "\n",
    "# Calcule o número de ligações necessário para atingir a porcentagem desejada\n",
    "num_ligacoes_necessarias = (porcentagem_desejada / 100) * total_clientes_interessados\n",
    "\n",
    "print(f\"Número de Ligações Necessárias para Contatar {porcentagem_desejada}% dos Clientes Interessados: {int(num_ligacoes_necessarias)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1. - Business Performance:\n",
    "In this section, we perform an analysis of the business performance based on the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Você deve adaptar o código abaixo para a maneira como carrega seus dados originais\n",
    "original_data = pd.read_csv('../propensity_score/data/raw/test.csv')\n",
    "\n",
    "# Agora, adicione a coluna 'id' de volta ao DataFrame df_test\n",
    "df_test['id'] = original_data['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calls = 20000\n",
    "total_leads = df_test.shape[0]\n",
    "percent_calls = calls / total_leads\n",
    "print('Insurance All total leads: {}'.format(total_leads))\n",
    "print('Insurance All % of calls: {0:.2f}%'.format(percent_calls * 100))\n",
    "\n",
    "customer_interesting = pd.DataFrame()\n",
    "df10 = df_test.copy()\n",
    "\n",
    "# Remova temporariamente a coluna 'id'\n",
    "df10.drop('id', axis=1, inplace=True)\n",
    "\n",
    "# Faça a previsão\n",
    "df10['response'] = rf_model_bp.predict_proba(df10)[:, 1]\n",
    "\n",
    "# Adicione a coluna 'id' de volta\n",
    "df10['id'] = df_test['id']\n",
    "\n",
    "probas = [df10['response']]\n",
    "customer_interesting = topK_performance(df10, probas, 'response', [percent_calls])\n",
    "customer_interesting['R$ model'] = customer_interesting['target_at_k'] * 2000\n",
    "customer_interesting['target_random'] = customer_interesting['perc'] * customer_interesting['target_total']\n",
    "customer_interesting['R$ random'] = customer_interesting['target_random'] * 2000\n",
    "customer_interesting['R$ Final'] = customer_interesting['R$ model'] - customer_interesting['R$ random']\n",
    "customer_interesting['X Final'] = customer_interesting['R$ model'] / customer_interesting['R$ random']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com base nos resultados, o modelo parece ser eficaz em identificar clientes com alta probabilidade de compra de um seguro de automóvel. Usando o modelo para direcionar suas ligações, pode-se esperar uma receita significativamente maior do que se fizesse ligações aleatórias. A receita final estimada é de aproximadamente 12.469.399,46 a mais do que em um cenário aleatório. Portanto, usar o modelo é uma estratégia eficaz para otimizar as vendas de seguros de automóvel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.0. STEP ELEVEN - DEPLOY MODEL TO PRODUCTION\n",
    "In this step, we will deploy the trained machine learning model to production so that it can be used for making predictions on new data. Deploying a model to production involves setting up a system or infrastructure that allows the model to receive input data, make predictions, and deliver the results to the end-users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Salve o dicionário do modelo em um arquivo pickle\n",
    "with open('rf_model_bp.pkl', 'wb') as file:\n",
    "    pickle.dump(rf_model_bp, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1. - Implementing a Insurance class:\n",
    "To implement the Insurance class, we'll follow the same structure provided above, but now we'll include the Flask app and the necessary methods to make predictions using the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import inflection\n",
    "import os \n",
    "\n",
    "from sklearn.ensemble     import RandomForestClassifier\n",
    "from sklearn              import preprocessing   as pp\n",
    "from lightgbm             import LGBMClassifier\n",
    "from sklearn              import metrics         as m\n",
    "from category_encoders    import OneHotEncoder\n",
    "\n",
    "class HealthInsurance (object):\n",
    "    \n",
    "    def __init__( self ):\n",
    "        # Use o caminho relativo para o diretório do script\n",
    "        script_dir = os.path.dirname(__file__)  # Obtém o diretório do script atual\n",
    "        self.home_path = script_dir  # Define o diretório como home_path\n",
    "\n",
    "        # Use caminhos relativos para carregar os arquivos pickle\n",
    "        self.gender_label_encoder = pickle.load(open (os.path.join(script_dir, 'parameter/gender_label_encoder.pkl'), 'rb'))\n",
    "        self.vehicle_damage_label_encoder = pickle.load(open('parameter/vehicle_damage_label_encoder.pkl','rb'))\n",
    "        self.mms_age = pickle.load(open(os.path.join(script_dir, 'parameter/mms_age_scaler.pkl'), 'rb'))\n",
    "        self.mms_annual_premium = pickle.load(open(os.path.join(script_dir, 'parameter/annual_premium_scaler.pkl'), 'rb'))\n",
    "        self.mms_vintage = pickle.load(open(os.path.join(script_dir, 'parameter/vintage_scaler.pkl'), 'rb'))\n",
    "        self.target_encode_region_code = pickle.load(open(os.path.join(script_dir, 'parameter/target_encode_region_code.pkl'), 'rb'))\n",
    "        self.fe_policy_sales_channel = pickle.load(open(os.path.join(script_dir, 'parameter/fe_policy_sales_channel_scaler.pkl'), 'rb'))\n",
    "    \n",
    "    def data_cleanning(self,df1):\n",
    "         # Renomeie as colunas para snake_case\n",
    "        df1.columns = [inflection.underscore(col) for col in df1.columns]\n",
    "        return df1\n",
    "                      \n",
    "    def feature_engieneering(self,df2):\n",
    "        df2['risk_age']= df2['age'].apply(lambda x: 0 if x>25 else 1)    \n",
    "        \n",
    "        df2['age_insured'] = ((df2['age'] >= 32) & (df2['age'] <= 52) & (df2['previously_insured'] == 0)).astype(int)\n",
    "\n",
    "        return df2\n",
    "    \n",
    "    def data_preparation(self,df5):\n",
    "        # # Carregue os LabelEncoders salvos\n",
    "        # gender_label_encoder = pickle.load(open('../propensity_score/parameter/gender_label_encoder.pkl', 'rb'))\n",
    "        # vehicle_damage_label_encoder = pickle.load(open('../propensity_score/parameter/vehicle_damage_label_encoder.pkl', 'rb'))\n",
    "        # # Carregue os MinMaxScalers salvos\n",
    "        # mms_age = pickle.load(open('../propensity_score/parameter/mms_age_scaler.pkl', 'rb'))\n",
    "        # mms_annual_premium = pickle.load(open('../propensity_score/parameter/annual_premium_scaler.pkl', 'rb'))\n",
    "        # mms_vintage = pickle.load(open('../propensity_score/parameter/vintage_scaler.pkl', 'rb'))\n",
    "        # # Carregue o dicionário de frequency encoding para 'policy_sales_channel'\n",
    "        # fe_policy_sales_channel = pickle.load(open('../propensity_score/parameter/fe_policy_sales_channel_scaler.pkl', 'rb'))\n",
    "        # # Carregue o dicionário de target encoding para 'region_code'\n",
    "        # target_encode_region_code = pickle.load(open('../propensity_score/parameter/target_encode_region_code.pkl', 'rb'))\n",
    "\n",
    "        # Aplique o Label Encoding às colunas apropriadas\n",
    "        df5['gender'] = self.gender_label_encoder.transform(df5['gender'])\n",
    "        df5['vehicle_damage'] = self.vehicle_damage_label_encoder.transform(df5['vehicle_damage'])\n",
    "\n",
    "        # Aplique o MinMaxScaler às colunas apropriadas\n",
    "        df5['age'] = self.mms_age.transform(df5[['age']])\n",
    "        df5['annual_premium'] = self.mms_annual_premium.transform(df5[['annual_premium']])\n",
    "        df5['vintage'] = self.mms_vintage.transform(df5[['vintage']])\n",
    "        \n",
    "        # Mapeie os valores em 'policy_sales_channel' com base nas frequências relativas\n",
    "        df5['policy_sales_channel'] = df5['policy_sales_channel'].map(self.fe_policy_sales_channel)\n",
    "\n",
    "        # Mapeie os valores em 'region_code' com base nas taxas de resposta média\n",
    "        df5['region_code'] = df5['region_code'].map(self.target_encode_region_code)\n",
    "\n",
    "        # Crie as colunas one-hot para 'vehicle_age'\n",
    "        df5['below_1_year'] = df5['vehicle_age'].apply(lambda x: 1 if x == '< 1 Year' else 0)\n",
    "        df5['between_1_2_year'] = df5['vehicle_age'].apply(lambda x: 1 if x == '1-2 Year' else 0)\n",
    "        df5['over_2_years'] = df5['vehicle_age'].apply(lambda x: 1 if x == '> 2 Years' else 0)\n",
    "        df5.drop('vehicle_age', axis=1, inplace=True)\n",
    "\n",
    "        df5 = df5.fillna(0)\n",
    "\n",
    "        # Feature Selection\n",
    "        cols_selected = ['annual_premium', 'vintage', 'age', 'region_code', 'vehicle_damage','policy_sales_channel',  'previously_insured', 'age_insured']\n",
    "        \n",
    "        return df5[ cols_selected ]\n",
    "\n",
    "    def get_prediction(self, model, original_data, test_data):\n",
    "        # model prediction\n",
    "        pred = model.predict_proba(test_data)\n",
    "        \n",
    "        # Extract the probabilities of the positive class (column 1)\n",
    "        positive_class_probabilities = pred[:, 1]\n",
    "        \n",
    "        # Create a new column 'prediction' in the original data\n",
    "        original_data['prediction'] = positive_class_probabilities\n",
    "        \n",
    "        return original_data.to_json(orient='records', date_format='iso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2. - API Handler:\n",
    "To implement the API Handler, we'll create a new Python script that will allow us to send test data to the API and receive the predictions. This script will simulate making requests to the API and receiving responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from flask import Flask,request,Response\n",
    "from healthinsurance import HealthInsurance\n",
    "\n",
    "model = pickle.load(open('rf_model_bp.pkl','rb'))\n",
    "\n",
    "app = Flask(__name__)\n",
    "@app.route('/healthinsurance/predict',methods=['POST'])\n",
    "\n",
    "def insurance_all_predict():\n",
    "    test_json = request.get_json()\n",
    "\n",
    "    if test_json: # there is data\n",
    "        if isinstance( test_json, dict ): # unique example\n",
    "            test_raw = pd.DataFrame( test_json, index=[0] )\n",
    "            \n",
    "        else: # multiple example\n",
    "            test_raw = pd.DataFrame( test_json, columns=test_json[0].keys() )\n",
    "            \n",
    "        # Instantiate Rossmann class\n",
    "        pipeline = HealthInsurance()\n",
    "        \n",
    "        # data cleaning\n",
    "        df1 = pipeline.data_cleanning( test_raw )\n",
    "        \n",
    "        # feature engineering\n",
    "        df2 = pipeline.feature_engieneering( df1 )\n",
    "        \n",
    "        # data preparation\n",
    "        df3 = pipeline.data_preparation( df2 )\n",
    "        \n",
    "        # prediction\n",
    "        df_response = pipeline.get_prediction( model, test_raw, df3 )\n",
    "        \n",
    "        return df_response\n",
    "    \n",
    "    else:\n",
    "        return Response( '{}', status=200, mimetype='application/json' )\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run('0.0.0.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3. -  API Tester:\n",
    "In this section, we are creating an API tester code to evaluate the performance of our deployed model. The API tester code sends test data to the API endpoint and retrieves the predictions returned by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# loading test dataset\n",
    "df_test = pd.read_csv( '../propensity_score/data/raw/test.csv' )\n",
    "\n",
    "# Convert dataframe to JSON\n",
    "data = json.dumps(df_test.to_dict(orient='records'))\n",
    "\n",
    "# API Call\n",
    "# url = 'http://127.0.0.1:5000/healthinsurance/predict'\n",
    "url = 'https://propensao-compra.onrender.com//healthinsurance/predict'\n",
    "header = {'Content-type': 'application/json' }\n",
    "\n",
    "r = requests.post( url, data=data, headers=header )\n",
    "print( 'Status Code {}'.format( r.status_code ) )\n",
    "\n",
    "d1 = pd.DataFrame( r.json(), columns=r.json()[0].keys() )\n",
    "d1.sort_values( 'prediction', ascending=False ).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Suponha que 'd1' seja o DataFrame resultante da resposta da API\n",
    "# 'd1' deve conter a coluna 'prediction' com as previsões no formato [probabilidade_classe_0, probabilidade_classe_1]\n",
    "\n",
    "# Função para reverter as transformações\n",
    "def revert_transformations(df):\n",
    "    # Reverter MinMaxScaler para a coluna 'age'\n",
    "    df['age'] = df['age'] * (max_age - min_age) + min_age\n",
    "    \n",
    "    # Reverter Label Encoding para a coluna 'gender'\n",
    "    df['gender'] = df['gender'].apply(lambda x: 'Male' if x == 1 else 'Female')\n",
    "    \n",
    "    # Reverter Label Encoding para a coluna 'vehicle_damage'\n",
    "    df['vehicle_damage'] = df['vehicle_damage'].apply(lambda x: 'Yes' if x == 1 else 'No')\n",
    "    \n",
    "    # Reverter MinMaxScaler para a coluna 'annual_premium'\n",
    "    mms_annual_premium = pickle.load(open('../compra_propensao/parameter/annual_premium_scaler.pkl', 'rb'))\n",
    "    df['annual_premium'] = mms_annual_premium.inverse_transform(df[['annual_premium']])\n",
    "\n",
    "    # Reverter MinMaxScaler para a coluna 'vintage'\n",
    "    mms_vintage = pickle.load(open('../compra_propensao/parameter/vintage_scaler.pkl', 'rb'))\n",
    "    df['vintage'] = mms_vintage.inverse_transform(df[['vintage']])\n",
    "\n",
    "    # Reverter as colunas one-hot para 'vehicle_age'\n",
    "    df['vehicle_age'] = df.apply(lambda row: '1-2 Year' if row['between_1_2_year'] == 1 else ('> 2 Years' if row['over_2_years'] == 1 else '< 1 Year'), axis=1)\n",
    "    df.drop(['below_1_year', 'between_1_2_year', 'over_2_years'], axis=1, inplace=True)\n",
    "\n",
    "    fe_policy_sales_channel = pickle.load(open('../compra_propensao/parameter/fe_policy_sales_channel_scaler.pkl', 'rb'))\n",
    "    target_encode_region_code = pickle.load(open('../compra_propensao/parameter/target_encode_region_code.pkl', 'rb'))\n",
    "\n",
    "    return df\n",
    "\n",
    "# MinMaxScaler - Valores mínimos e máximos da coluna 'age' após a transformação\n",
    "min_age = 20 \n",
    "max_age = 85\n",
    "\n",
    "# Use a função para reverter as transformações\n",
    "d1_reverted = revert_transformations(d1.copy())  # Certifique-se de copiar o DataFrame original para evitar alterações indesejadas\n",
    "\n",
    "d1_reverted.head(10) # 'd1_reverted' agora contém os dados revertidos ao estado original"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projeto_ds_producao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
